{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xt1317y2ixSS",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:03:47.098438200Z",
     "start_time": "2023-08-23T06:03:47.035015800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl import config_tickers\n",
    "from finrl.config import INDICATORS\n",
    "\n",
    "import itertools\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from Envs.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mFCP1YEhi6oi",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:03:53.159497100Z",
     "start_time": "2023-08-23T06:03:52.910244300Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('all_data_with_sentiment_FINAL.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "#train = train.set_index(train.columns[0])\n",
    "#train.index.names = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date   tic     open     high      low    close     volume  \\\n",
      "0  2021-11-24 14:00:00  AAPL  159.380  159.703  158.862  159.085  3513469.0   \n",
      "1  2021-11-24 14:00:00   PFE   47.758   47.831   47.607   47.735   686904.0   \n",
      "2  2021-11-24 14:00:00    MO   38.575   38.598   38.350   38.398   295661.0   \n",
      "3  2021-11-24 14:00:00  META  340.290  341.370  339.000  339.175   695874.0   \n",
      "4  2021-11-24 14:00:00    PG  141.959  142.221  141.802  142.113   230089.0   \n",
      "\n",
      "   day  macd     boll_ub  ...  dx_30  close_30_sma  close_60_sma     vix  \\\n",
      "0  5.0   0.0  160.308182  ...  100.0       159.085       159.085  85.622   \n",
      "1  5.0   0.0  158.835572  ...  100.0        47.735        47.735  85.622   \n",
      "2  5.0   0.0  160.308182  ...  100.0        38.398        38.398  85.622   \n",
      "3  5.0   0.0  160.308182  ...  100.0       339.175       339.175  85.622   \n",
      "4  5.0   0.0  142.570497  ...  100.0       142.113       142.113  85.622   \n",
      "\n",
      "     change  stocktwitsPosts  stocktwitsLikes  stocktwitsImpressions  \\\n",
      "0 -0.591471            102.0            132.0               361612.0   \n",
      "1 -1.960851             14.0              6.0                19312.0   \n",
      "2 -2.130297              0.0              0.0                    0.0   \n",
      "3  3.019949             34.0             18.0                86157.0   \n",
      "4 -0.227013              0.0              0.0                    0.0   \n",
      "\n",
      "   stocktwitsSentiment    random  \n",
      "0             0.532856  0.296856  \n",
      "1             0.476562  0.258228  \n",
      "2             0.313871  0.877339  \n",
      "3             0.520818  0.786928  \n",
      "4             0.307690  0.369543  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add a new column 'random' with random values between 0 and 1\n",
    "train['random'] = np.random.rand(len(train))\n",
    "\n",
    "print(train.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "SENTIMENT = ['stocktwitsPosts','stocktwitsLikes','stocktwitsImpressions','stocktwitsSentiment', 'random']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to C:\\Users\\potap\\FinARL\\automatic-stock-trading-applied-reinforcement-learning-sommer-term-2023\\Generated Sentiment Data in FinRL\\stock_data\\all_data_with_index.csv\n",
      "                      date   tic     open     high      low    close  \\\n",
      "index                                                                  \n",
      "0      2021-11-24 14:00:00  AAPL  159.380  159.703  158.862  159.085   \n",
      "0      2021-11-24 14:00:00   NKE  168.081  168.315  167.650  168.123   \n",
      "0      2021-11-24 14:00:00   PEP  154.583  154.732  154.265  154.514   \n",
      "0      2021-11-24 14:00:00  NVDA  323.761  325.132  321.596  323.210   \n",
      "0      2021-11-24 14:00:00  NFLX  659.920  660.880  656.890  657.750   \n",
      "\n",
      "          volume  day  macd     boll_ub  ...  dx_30  close_30_sma  \\\n",
      "index                                    ...                        \n",
      "0      3513469.0  5.0   0.0  160.308182  ...  100.0       159.085   \n",
      "0       164604.0  5.0   0.0  168.815945  ...  100.0       168.123   \n",
      "0        88332.0  5.0   0.0  158.835572  ...  100.0       154.514   \n",
      "0      1677597.0  5.0   0.0  160.308182  ...  100.0       323.210   \n",
      "0        46157.0  5.0   0.0  660.353330  ...  100.0       657.750   \n",
      "\n",
      "       close_60_sma     vix    change  stocktwitsPosts  stocktwitsLikes  \\\n",
      "index                                                                     \n",
      "0           159.085  85.622 -0.591471            102.0            132.0   \n",
      "0           168.123  85.622 -1.364129              1.0              0.0   \n",
      "0           154.514  85.622  1.392279              1.0              0.0   \n",
      "0           323.210  85.622  0.983951             71.0             59.0   \n",
      "0           657.750  85.622  1.448540              4.0              0.0   \n",
      "\n",
      "       stocktwitsImpressions  stocktwitsSentiment    random  \n",
      "index                                                        \n",
      "0                   361612.0             0.532856  0.296856  \n",
      "0                     2031.0             0.366283  0.907451  \n",
      "0                     8167.0             0.301477  0.382025  \n",
      "0                   139229.0             0.522796  0.403868  \n",
      "0                      653.0             0.496894  0.565796  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame by the 'date' column\n",
    "train = train.sort_values(by='date')\n",
    "\n",
    "# Create a mapping of unique dates to their corresponding index\n",
    "date_mapping = {date: idx for idx, date in enumerate(train['date'].unique())}\n",
    "\n",
    "# Map the 'date' column to the date_mapping to get the new index\n",
    "train['index'] = train['date'].map(date_mapping)\n",
    "\n",
    "# Set the new index for the DataFrame\n",
    "train.set_index('index', inplace=True, drop=True)\n",
    "# Specify the path where you want to save the CSV file\n",
    "output_path = r\"C:\\Users\\potap\\FinARL\\automatic-stock-trading-applied-reinforcement-learning-sommer-term-2023\\Generated Sentiment Data in FinRL\\stock_data\\all_data_with_index.csv\"\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "train.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {output_path}\")\n",
    "print(train.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:03:53.525717700Z",
     "start_time": "2023-08-23T06:03:53.446808900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "date                      object\ntic                       object\nopen                     float64\nhigh                     float64\nlow                      float64\nclose                    float64\nvolume                   float64\nday                      float64\nmacd                     float64\nboll_ub                  float64\nboll_lb                  float64\nrsi_30                   float64\ncci_30                   float64\ndx_30                    float64\nclose_30_sma             float64\nclose_60_sma             float64\nvix                      float64\nchange                   float64\nstocktwitsPosts          float64\nstocktwitsLikes          float64\nstocktwitsImpressions    float64\nstocktwitsSentiment      float64\nrandom                   float64\ndtype: object"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.fillna(0)\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#SENTIMENT = ['stocktwitsPosts','stocktwitsLikes','stocktwitsImpressions','stocktwitsSentiment']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#SENTIMENT = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "INDICATORS.append('vix')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:01.972279700Z",
     "start_time": "2023-08-23T06:06:01.956637300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 30, State Space: 541\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 4*stock_dimension + len(INDICATORS)*stock_dimension +  len(SENTIMENT)*stock_dimension\n",
    "#state_space = 379\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['stocktwitsPosts',\n 'stocktwitsLikes',\n 'stocktwitsImpressions',\n 'stocktwitsSentiment',\n 'random']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTIMENT"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:02.969957700Z",
     "start_time": "2023-08-23T06:06:02.953981700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['macd',\n 'boll_ub',\n 'boll_lb',\n 'rsi_30',\n 'cci_30',\n 'dx_30',\n 'close_30_sma',\n 'close_60_sma',\n 'vix']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDICATORS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"sentiment_list\" : SENTIMENT,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:05.541279300Z",
     "start_time": "2023-08-23T06:06:05.494015200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "364PsqckttcQ",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:11.154422Z",
     "start_time": "2023-08-23T06:06:11.123593400Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:14.020357900Z",
     "start_time": "2023-08-23T06:06:13.958069700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:32.365749900Z",
     "start_time": "2023-08-23T06:06:15.670698500Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0.0051    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -3.61e+03 |\n",
      "|    reward             | 27.20757  |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.24e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 189        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.00398    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -2.03e+03  |\n",
      "|    reward             | 103.201004 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 2.88e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -0.00647   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -3.79e+03  |\n",
      "|    reward             | -72.435005 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.78e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00216  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 2.95e+03  |\n",
      "|    reward             | 188.03972 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.23e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.0069     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 1.68e+03   |\n",
      "|    reward             | -185.11946 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 9.3e+03    |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 191      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | -0.00418 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 8.75e+03 |\n",
      "|    reward             | 346.3712 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 5.99e+04 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0.0344    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 342       |\n",
      "|    reward             | -12.49053 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 611       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00758  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -2.89e+03 |\n",
      "|    reward             | 25.009933 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 6.36e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 191      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | 0.000804 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 2.82e+03 |\n",
      "|    reward             | 69.97375 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 9e+03    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0.0209     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 3.39e+03   |\n",
      "|    reward             | 125.349396 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.81e+04   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -0.0112    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 1.39e+03   |\n",
      "|    reward             | -248.81761 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 1.26e+04   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 193      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.5    |\n",
      "|    explained_variance | -0.00387 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 1.29e+04 |\n",
      "|    reward             | 86.69685 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 1.06e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.0174   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -1.72e+03 |\n",
      "|    reward             | 93.77996  |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 3.15e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.0154   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 2.94e+03  |\n",
      "|    reward             | 32.645756 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.47e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.00748    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -5.89e+03  |\n",
      "|    reward             | -44.672146 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 4.2e+04    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | -0.00689   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 4.5e+03    |\n",
      "|    reward             | -235.35306 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.57e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00239  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 6.7e+03   |\n",
      "|    reward             | 48.044712 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 6.36e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.000473 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 9.61e+03  |\n",
      "|    reward             | 586.10474 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.53e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 192      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | -0.0128  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 990      |\n",
      "|    reward             | 23.91629 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.85e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -0.0441   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 68.2      |\n",
      "|    reward             | -39.83408 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.27e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0.0363    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -2.95e+03 |\n",
      "|    reward             | 117.05595 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.69e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -0.0173   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 9.25      |\n",
      "|    reward             | 144.07942 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.12e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 190        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0.015      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | -2.72e+03  |\n",
      "|    reward             | -222.33862 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 3.99e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00783  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | -3.99e+03 |\n",
      "|    reward             | -367.0017 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 7.55e+04  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 65         |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -0.0817    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | 2.36       |\n",
      "|    reward             | -47.556377 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 1.06e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | -0.00125  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | -125      |\n",
      "|    reward             | -209.4224 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 4.54e+03  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=500000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjCWfgsg3sVa",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 45         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 44         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.31128678 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015403882 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0119     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.48        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    reward               | 0.5735508   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 15          |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3175090.47\n",
      "total_reward: 2175090.47\n",
      "total_cost: 337808.39\n",
      "total_trades: 80668\n",
      "Sharpe: 0.689\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015230103 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00311     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    reward               | -1.4208665  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 65.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019708812 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00785    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    reward               | 4.271296    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 45.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 44        |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 231       |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0166731 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -41.4     |\n",
      "|    explained_variance   | -0.0536   |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 8.35      |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.0212   |\n",
      "|    reward               | 2.4547098 |\n",
      "|    std                  | 1.01      |\n",
      "|    value_loss           | 19.9      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 279         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018124811 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.000224    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    reward               | 2.7911348   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 76.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 325         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024045099 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.00731    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.2        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    reward               | 1.2000475   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 85.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 401         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020597177 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.00494    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.9        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    reward               | 1.162485    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 39          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 457         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019878255 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.00339     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    reward               | -0.45392472 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 57          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 498         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017024204 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | -0.0128     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.4        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    reward               | 1.0464087   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 79.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 542         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019309249 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.00221     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 41.1        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    reward               | 4.342377    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 90.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 595         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023128178 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | -0.0209     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.7        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | -0.17601006 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 52.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 644         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.0221886   |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.00158     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    reward               | -0.01640089 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 87.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 686         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021495752 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.00679     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00989    |\n",
      "|    reward               | -3.5598922  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 730         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020674404 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.0104      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    reward               | 6.633994    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 50.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 780         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020497315 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.00979     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 55.1        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | 0.8197784   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 76          |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5135262.34\n",
      "total_reward: 4135262.34\n",
      "total_cost: 287991.52\n",
      "total_trades: 76232\n",
      "Sharpe: 0.825\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 824         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016542673 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.0115      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 57.3        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00886    |\n",
      "|    reward               | 0.7494548   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 902         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015852435 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.00322     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 53.8        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00988    |\n",
      "|    reward               | 0.030309012 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 948         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018456738 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | 0.0132      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23          |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    reward               | 3.2421417   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 59.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 41         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 991        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02275483 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42        |\n",
      "|    explained_variance   | 0.00528    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 75.2       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.00876   |\n",
      "|    reward               | 0.12529488 |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 120        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 1038        |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016805276 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.0179      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 91.7        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00777    |\n",
      "|    reward               | -7.0438967  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 151         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 1084        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023760147 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.0146      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23          |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.007      |\n",
      "|    reward               | 2.0401194   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 51.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1128        |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020118829 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.0153      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.2        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    reward               | 0.71143794  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 77.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1174        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016950518 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.0019      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 48.3        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    reward               | 6.65837     |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 1221        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015065431 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.000935    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 63          |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    reward               | 0.15126267  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 41         |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 1270       |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02162559 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.3      |\n",
      "|    explained_variance   | -0.051     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 12.2       |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.00781   |\n",
      "|    reward               | 2.7743616  |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 35.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 1316        |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018873222 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | -0.00448    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 150         |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    reward               | -0.3141937  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 403         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 1365        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019432027 |\n",
      "|    clip_fraction        | 0.0966      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | -0.00524    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 154         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    reward               | -3.5503688  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 576         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 1415        |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016036455 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.0583      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0097     |\n",
      "|    reward               | -0.7532961  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 28          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 1465        |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020757344 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.00572     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 51.6        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    reward               | 1.4703931   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 94.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 1526        |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011127746 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0169      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 231         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | -1.467241   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 210         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3243555.19\n",
      "total_reward: 2243555.19\n",
      "total_cost: 285552.40\n",
      "total_trades: 75997\n",
      "Sharpe: 0.630\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 1580        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016851306 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0239      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.8        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00735    |\n",
      "|    reward               | 0.46168545  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 38.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 1635        |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020944998 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0675      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 40.7        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    reward               | -0.6691587  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 98          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 1690        |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019425632 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.00129     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 683         |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    reward               | 0.7678009   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 779         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 1749        |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029058285 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.0135      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.1        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    reward               | 1.0060796   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 92.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 1797        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020738672 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0737      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | -3.2037091  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 26.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 1827        |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02297673  |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0195      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 81.3        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    reward               | -0.07032298 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 41         |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 1855       |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02278569 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.0599     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 58         |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    reward               | -8.025124  |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 56         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 1883        |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012080987 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0106      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.7        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    reward               | -1.7869323  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 33.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 1912        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018431693 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0313      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | -0.90353096 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 70.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 43         |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 1941       |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03000924 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.0217     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 36.4       |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.00702   |\n",
      "|    reward               | 0.6959806  |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 66         |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 1969        |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02106736  |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0226      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.9        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00763    |\n",
      "|    reward               | -0.07551384 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 78.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 44         |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 1997       |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02621992 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.9      |\n",
      "|    explained_variance   | 0.0432     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 6.73       |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    reward               | -0.5257798 |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 16.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 2029        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018179985 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0481      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0094     |\n",
      "|    reward               | 0.07973856  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 50.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 2057        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025584307 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.0137      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.7        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | -1.5210438  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 45.8        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2434835.58\n",
      "total_reward: 1434835.58\n",
      "total_cost: 292094.35\n",
      "total_trades: 75755\n",
      "Sharpe: 0.531\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 2085        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020613633 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | -0.0237     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    reward               | -6.6412554  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 20          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 2115        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051772773 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.0464      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | 0.9050805   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 28.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 45         |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 2145       |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02411993 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.2      |\n",
      "|    explained_variance   | 0.0566     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 24.1       |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    reward               | 8.264698   |\n",
      "|    std                  | 1.07       |\n",
      "|    value_loss           | 77.9       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 2177         |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0139401145 |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.2        |\n",
      "|    explained_variance   | 0.042        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 61           |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    reward               | 0.5889933    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 99.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 2209        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017555572 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.00983     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.24        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00979    |\n",
      "|    reward               | 1.2748959   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 27          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 2240        |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024905618 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.0205      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 107         |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    reward               | -0.38472286 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 183         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 2272        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008009402 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.00851     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 110         |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | -8.162935   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 358         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 2303        |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011461023 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.0416      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 33.1        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | 0.9152776   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 70.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 2335        |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016434327 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.00823     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 157         |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00802    |\n",
      "|    reward               | 2.981041    |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 383         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 2365        |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016391305 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0388      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 152         |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00422    |\n",
      "|    reward               | 6.4678645   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 327         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 2396        |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012791837 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0564      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 72.7        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00704    |\n",
      "|    reward               | 3.231686    |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 161         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 2428        |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018063571 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0386      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 103         |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | -0.81493115 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 293         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 2457        |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014747455 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 47.7        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00979    |\n",
      "|    reward               | 0.52013004  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 222         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 2488        |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021737006 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.0135      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 70.8        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00868    |\n",
      "|    reward               | -7.884881   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4069059.76\n",
      "total_reward: 3069059.76\n",
      "total_cost: 264245.13\n",
      "total_trades: 72011\n",
      "Sharpe: 0.683\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 2517        |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056785204 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | 0.00132     |\n",
      "|    reward               | -0.21071224 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 36.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 2547        |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017078368 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.0117      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 82          |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00869    |\n",
      "|    reward               | 1.1230668   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 218         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 2577        |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027008304 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.0505      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 43.6        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00935    |\n",
      "|    reward               | 6.6512985   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 66.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 2609        |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039094795 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.0596      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00381    |\n",
      "|    reward               | 3.7435694   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 37.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 2640        |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023921957 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.0723      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.3        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    reward               | 0.22937617  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 86.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 49         |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 2670       |\n",
      "|    total_timesteps      | 133120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01888536 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.8      |\n",
      "|    explained_variance   | 0.126      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 27.6       |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    reward               | -0.9992001 |\n",
      "|    std                  | 1.1        |\n",
      "|    value_loss           | 87.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 2700        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024767734 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 87.1        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    reward               | 3.9526186   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 162         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 2729        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060827535 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.0572      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.93        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | 0.00566     |\n",
      "|    reward               | -1.1473198  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 21.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 2760        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014017103 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 53.8        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00271    |\n",
      "|    reward               | -0.07142399 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 72.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 2791        |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020744808 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.153       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.2        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00391    |\n",
      "|    reward               | 0.70842856  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 59          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 50         |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 2822       |\n",
      "|    total_timesteps      | 143360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02585392 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44        |\n",
      "|    explained_variance   | 0.0351     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 13.1       |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.00977   |\n",
      "|    reward               | -1.950569  |\n",
      "|    std                  | 1.11       |\n",
      "|    value_loss           | 29.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 2852        |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032436244 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 48.5        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00307    |\n",
      "|    reward               | -1.1932291  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 84.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 2883        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015441446 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.3        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0051     |\n",
      "|    reward               | -9.417746   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 69.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 2913        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012106962 |\n",
      "|    clip_fraction        | 0.0806      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00548    |\n",
      "|    reward               | 0.49566022  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 42.9        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1980690.34\n",
      "total_reward: 980690.34\n",
      "total_cost: 169265.73\n",
      "total_trades: 66162\n",
      "Sharpe: 0.369\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 2943        |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021132182 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.0802      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.5         |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    reward               | -1.626163   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 36.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 2973        |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024074746 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.3        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0007     |\n",
      "|    reward               | 1.8852057   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 48.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 3003        |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019135812 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 75.2        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    reward               | 4.1208997   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 3033        |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049443997 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.0738      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.9        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00461    |\n",
      "|    reward               | -0.5149379  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 31.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 3065        |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017629089 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 31.2        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00908    |\n",
      "|    reward               | 2.1882532   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 81.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 52         |\n",
      "|    iterations           | 79         |\n",
      "|    time_elapsed         | 3094       |\n",
      "|    total_timesteps      | 161792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01690145 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.3      |\n",
      "|    explained_variance   | 0.155      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 42.8       |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.00397   |\n",
      "|    reward               | 8.179026   |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 108        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 3124        |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028971553 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.0989      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.7        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    reward               | 0.30852684  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 65          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 3152        |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019610118 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.0958      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00749    |\n",
      "|    reward               | -0.4584207  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 3180        |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015684314 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.0692      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 35.4        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    reward               | -0.6564304  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 157         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 3209        |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021742439 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.0788      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    reward               | 0.8954489   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 3237        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032015637 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.00259     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.17        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    reward               | -0.3945083  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 85         |\n",
      "|    time_elapsed         | 3264       |\n",
      "|    total_timesteps      | 174080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05891064 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.4      |\n",
      "|    explained_variance   | 0.0777     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 15.1       |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0085    |\n",
      "|    reward               | 0.49847165 |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 58.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 3291        |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030904215 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.082       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | -0.07042978 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 77.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 3318        |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050372817 |\n",
      "|    clip_fraction        | 0.459       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.034       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.26        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | 0.00687     |\n",
      "|    reward               | 2.851221    |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 21.8        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2677060.35\n",
      "total_reward: 1677060.35\n",
      "total_cost: 269589.12\n",
      "total_trades: 72774\n",
      "Sharpe: 0.583\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 3346        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028356094 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.2        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    reward               | -2.7264383  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 35.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 54           |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 3373         |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.05962711   |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.6        |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 20.5         |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | 0.00107      |\n",
      "|    reward               | -0.056611504 |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 36.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 3402        |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024442453 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.0674      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00332    |\n",
      "|    reward               | -0.41769487 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 43          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 3433        |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034742087 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | -0.00378    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.72        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    reward               | -0.20895879 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 14          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 3463        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031162115 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.0296      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 62.7        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00606    |\n",
      "|    reward               | -2.0798807  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 68.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 3490       |\n",
      "|    total_timesteps      | 190464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04095839 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.9      |\n",
      "|    explained_variance   | -0.000793  |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 25.5       |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    reward               | -1.9883596 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 89.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 3519        |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023274139 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | -0.0252     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.7        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00922    |\n",
      "|    reward               | 0.616459    |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 35.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 3547        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030782362 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.00162     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 96.8        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    reward               | 0.033399694 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 154         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 3575        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027262196 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 42.3        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    reward               | 2.1065326   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 68.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 3603       |\n",
      "|    total_timesteps      | 198656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04975403 |\n",
      "|    clip_fraction        | 0.407      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45        |\n",
      "|    explained_variance   | 0.0158     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 15.2       |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -9.3e-05   |\n",
      "|    reward               | 0.73203063 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 31.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 3632        |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028849795 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.0384      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 31.8        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00741    |\n",
      "|    reward               | 0.098063245 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 53.4        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
