{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xt1317y2ixSS",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:03:47.098438200Z",
     "start_time": "2023-08-23T06:03:47.035015800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl import config_tickers\n",
    "from finrl.config import INDICATORS\n",
    "\n",
    "import itertools\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from Envs.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mFCP1YEhi6oi",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:03:53.159497100Z",
     "start_time": "2023-08-23T06:03:52.910244300Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:03:53.525717700Z",
     "start_time": "2023-08-23T06:03:53.446808900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            date   tic        open        high         low       close  \\\n                                                                         \n0     2009-01-02  AAPL    3.067143    3.251429    3.041429    2.754724   \n0     2009-01-02  AMGN   58.590000   59.080002   57.750000   43.422924   \n0     2009-01-02   AXP   18.570000   19.520000   18.400000   15.256270   \n0     2009-01-02    BA   42.799999   45.560001   42.779999   33.941090   \n0     2009-01-02   CAT   44.910000   46.980000   44.709999   31.254068   \n...          ...   ...         ...         ...         ...         ...   \n2892  2020-06-30   UNH  288.570007  296.450012  287.660004  282.806396   \n2892  2020-06-30     V  191.490005  193.750000  190.160004  189.235092   \n2892  2020-06-30    VZ   54.919998   55.290001   54.360001   46.551456   \n2892  2020-06-30   WBA   42.119999   42.580002   41.759998   37.066734   \n2892  2020-06-30   WMT  119.220001  120.129997  118.540001  114.279716   \n\n           volume  day      macd     boll_ub     boll_lb      rsi_30  \\\n                                                                       \n0     746015200.0  4.0  0.000000    2.977272    2.648437  100.000000   \n0       6547900.0  4.0  0.000000    2.977272    2.648437  100.000000   \n0      10955700.0  4.0  0.000000    2.977272    2.648437  100.000000   \n0       7010200.0  4.0  0.000000    2.977272    2.648437  100.000000   \n0       7117200.0  4.0  0.000000    2.977272    2.648437  100.000000   \n...           ...  ...       ...         ...         ...         ...   \n2892    2932900.0  1.0 -0.019138  298.676589  266.566270   52.413041   \n2892    9040100.0  1.0  1.040529  197.185265  183.584111   53.021037   \n2892   17414800.0  1.0 -0.403920   49.824212   45.029134   48.097028   \n2892    4782100.0  1.0 -0.079749   40.460045   34.646651   48.830191   \n2892    6836400.0  1.0 -0.872506  117.578538  111.709822   48.159670   \n\n         cci_30       dx_30  close_30_sma  close_60_sma        vix  turbulence  \n                                                                                \n0     66.666667  100.000000      2.754724      2.754724  39.189999    0.000000  \n0     66.666667  100.000000     43.422924     43.422924  39.189999    0.000000  \n0     66.666667  100.000000     15.256270     15.256270  39.189999    0.000000  \n0     66.666667  100.000000     33.941090     33.941090  39.189999    0.000000  \n0     66.666667  100.000000     31.254068     31.254068  39.189999    0.000000  \n...         ...         ...           ...           ...        ...         ...  \n2892 -25.974551    1.846804    283.046084    276.148069  30.430000   12.918778  \n2892 -51.626992    2.013358    189.976997    180.246881  30.430000   12.918778  \n2892 -51.313221    8.508886     47.138594     47.556786  30.430000   12.918778  \n2892 -14.655975    1.500723     37.161170     36.971200  30.430000   12.918778  \n2892 -69.989973    3.847271    115.919150    117.824090  30.430000   12.918778  \n\n[83897 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>tic</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>day</th>\n      <th>macd</th>\n      <th>boll_ub</th>\n      <th>boll_lb</th>\n      <th>rsi_30</th>\n      <th>cci_30</th>\n      <th>dx_30</th>\n      <th>close_30_sma</th>\n      <th>close_60_sma</th>\n      <th>vix</th>\n      <th>turbulence</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>AAPL</td>\n      <td>3.067143</td>\n      <td>3.251429</td>\n      <td>3.041429</td>\n      <td>2.754724</td>\n      <td>746015200.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.977272</td>\n      <td>2.648437</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>2.754724</td>\n      <td>2.754724</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>AMGN</td>\n      <td>58.590000</td>\n      <td>59.080002</td>\n      <td>57.750000</td>\n      <td>43.422924</td>\n      <td>6547900.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.977272</td>\n      <td>2.648437</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>43.422924</td>\n      <td>43.422924</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>AXP</td>\n      <td>18.570000</td>\n      <td>19.520000</td>\n      <td>18.400000</td>\n      <td>15.256270</td>\n      <td>10955700.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.977272</td>\n      <td>2.648437</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>15.256270</td>\n      <td>15.256270</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>BA</td>\n      <td>42.799999</td>\n      <td>45.560001</td>\n      <td>42.779999</td>\n      <td>33.941090</td>\n      <td>7010200.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.977272</td>\n      <td>2.648437</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>33.941090</td>\n      <td>33.941090</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>CAT</td>\n      <td>44.910000</td>\n      <td>46.980000</td>\n      <td>44.709999</td>\n      <td>31.254068</td>\n      <td>7117200.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.977272</td>\n      <td>2.648437</td>\n      <td>100.000000</td>\n      <td>66.666667</td>\n      <td>100.000000</td>\n      <td>31.254068</td>\n      <td>31.254068</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>2020-06-30</td>\n      <td>UNH</td>\n      <td>288.570007</td>\n      <td>296.450012</td>\n      <td>287.660004</td>\n      <td>282.806396</td>\n      <td>2932900.0</td>\n      <td>1.0</td>\n      <td>-0.019138</td>\n      <td>298.676589</td>\n      <td>266.566270</td>\n      <td>52.413041</td>\n      <td>-25.974551</td>\n      <td>1.846804</td>\n      <td>283.046084</td>\n      <td>276.148069</td>\n      <td>30.430000</td>\n      <td>12.918778</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>2020-06-30</td>\n      <td>V</td>\n      <td>191.490005</td>\n      <td>193.750000</td>\n      <td>190.160004</td>\n      <td>189.235092</td>\n      <td>9040100.0</td>\n      <td>1.0</td>\n      <td>1.040529</td>\n      <td>197.185265</td>\n      <td>183.584111</td>\n      <td>53.021037</td>\n      <td>-51.626992</td>\n      <td>2.013358</td>\n      <td>189.976997</td>\n      <td>180.246881</td>\n      <td>30.430000</td>\n      <td>12.918778</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>2020-06-30</td>\n      <td>VZ</td>\n      <td>54.919998</td>\n      <td>55.290001</td>\n      <td>54.360001</td>\n      <td>46.551456</td>\n      <td>17414800.0</td>\n      <td>1.0</td>\n      <td>-0.403920</td>\n      <td>49.824212</td>\n      <td>45.029134</td>\n      <td>48.097028</td>\n      <td>-51.313221</td>\n      <td>8.508886</td>\n      <td>47.138594</td>\n      <td>47.556786</td>\n      <td>30.430000</td>\n      <td>12.918778</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>2020-06-30</td>\n      <td>WBA</td>\n      <td>42.119999</td>\n      <td>42.580002</td>\n      <td>41.759998</td>\n      <td>37.066734</td>\n      <td>4782100.0</td>\n      <td>1.0</td>\n      <td>-0.079749</td>\n      <td>40.460045</td>\n      <td>34.646651</td>\n      <td>48.830191</td>\n      <td>-14.655975</td>\n      <td>1.500723</td>\n      <td>37.161170</td>\n      <td>36.971200</td>\n      <td>30.430000</td>\n      <td>12.918778</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>2020-06-30</td>\n      <td>WMT</td>\n      <td>119.220001</td>\n      <td>120.129997</td>\n      <td>118.540001</td>\n      <td>114.279716</td>\n      <td>6836400.0</td>\n      <td>1.0</td>\n      <td>-0.872506</td>\n      <td>117.578538</td>\n      <td>111.709822</td>\n      <td>48.159670</td>\n      <td>-69.989973</td>\n      <td>3.847271</td>\n      <td>115.919150</td>\n      <td>117.824090</td>\n      <td>30.430000</td>\n      <td>12.918778</td>\n    </tr>\n  </tbody>\n</table>\n<p>83897 rows × 18 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal for generated data: one more column for sentiment for each day --> Merge on \"tic\" and \"date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:03:55.934833700Z",
     "start_time": "2023-08-23T06:03:55.889770200Z"
    }
   },
   "outputs": [],
   "source": [
    "jason = [{\n",
    "    \"date\" : \"2012-06-27\",\n",
    "    \"symbol\" : \"AAPL\",\n",
    "    \"stocktwitsPosts\" : 13,\n",
    "    \"twitterPosts\" : 163,\n",
    "    \"stocktwitsComments\" : 9,\n",
    "    \"twitterComments\" : 7769,\n",
    "    \"stocktwitsLikes\" : 16,\n",
    "    \"twitterLikes\" : 40957,\n",
    "    \"stocktwitsImpressions\" : 15141,\n",
    "    \"twitterImpressions\" : 1576854,\n",
    "    \"stocktwitsSentiment\" : 0.5411,\n",
    "    \"twitterSentiment\" : 0.5888\n",
    "  }, {\n",
    "    \"date\" : \"2012-06-28\",\n",
    "    \"symbol\" : \"AAPL\",\n",
    "    \"stocktwitsPosts\" : 19,\n",
    "    \"twitterPosts\" : 76,\n",
    "    \"stocktwitsComments\" : 8,\n",
    "    \"twitterComments\" : 1206,\n",
    "    \"stocktwitsLikes\" : 47,\n",
    "    \"twitterLikes\" : 8909,\n",
    "    \"stocktwitsImpressions\" : 51534,\n",
    "    \"twitterImpressions\" : 327871,\n",
    "    \"stocktwitsSentiment\" : 0.5518,\n",
    "    \"twitterSentiment\" : 0.5299\n",
    "  },\n",
    "    {\n",
    "    \"date\" : \"2012-06-29\",\n",
    "    \"symbol\" : \"AAPL\",\n",
    "    \"stocktwitsPosts\" : 13,\n",
    "    \"twitterPosts\" : 163,\n",
    "    \"stocktwitsComments\" : 9,\n",
    "    \"twitterComments\" : 7769,\n",
    "    \"stocktwitsLikes\" : 16,\n",
    "    \"twitterLikes\" : 40957,\n",
    "    \"stocktwitsImpressions\" : 15141,\n",
    "    \"twitterImpressions\" : 1576854,\n",
    "    \"stocktwitsSentiment\" : 0.5411,\n",
    "    \"twitterSentiment\" : 0.5888\n",
    "  }, {\n",
    "    \"date\" : \"2012-06-30\",\n",
    "    \"symbol\" : \"AAPL\",\n",
    "    \"stocktwitsPosts\" : 19,\n",
    "    \"twitterPosts\" : 76,\n",
    "    \"stocktwitsComments\" : 8,\n",
    "    \"twitterComments\" : 1206,\n",
    "    \"stocktwitsLikes\" : 47,\n",
    "    \"twitterLikes\" : 8909,\n",
    "    \"stocktwitsImpressions\" : 51534,\n",
    "    \"twitterImpressions\" : 327871,\n",
    "    \"stocktwitsSentiment\" : 0.5518,\n",
    "    \"twitterSentiment\" : 0.5299\n",
    "  }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:04:26.117248900Z",
     "start_time": "2023-08-23T06:04:26.039138500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         date   tic  stocktwitsPosts  twitterPosts  stocktwitsComments  \\\n0  2012-06-27  AAPL               13           163                   9   \n1  2012-06-28  AAPL               19            76                   8   \n2  2012-06-29  AAPL               13           163                   9   \n3  2012-06-30  AAPL               19            76                   8   \n\n   twitterComments  stocktwitsLikes  twitterLikes  stocktwitsImpressions  \\\n0             7769               16         40957                  15141   \n1             1206               47          8909                  51534   \n2             7769               16         40957                  15141   \n3             1206               47          8909                  51534   \n\n   twitterImpressions  stocktwitsSentiment  twitterSentiment  \n0             1576854               0.5411            0.5888  \n1              327871               0.5518            0.5299  \n2             1576854               0.5411            0.5888  \n3              327871               0.5518            0.5299  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>tic</th>\n      <th>stocktwitsPosts</th>\n      <th>twitterPosts</th>\n      <th>stocktwitsComments</th>\n      <th>twitterComments</th>\n      <th>stocktwitsLikes</th>\n      <th>twitterLikes</th>\n      <th>stocktwitsImpressions</th>\n      <th>twitterImpressions</th>\n      <th>stocktwitsSentiment</th>\n      <th>twitterSentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2012-06-27</td>\n      <td>AAPL</td>\n      <td>13</td>\n      <td>163</td>\n      <td>9</td>\n      <td>7769</td>\n      <td>16</td>\n      <td>40957</td>\n      <td>15141</td>\n      <td>1576854</td>\n      <td>0.5411</td>\n      <td>0.5888</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2012-06-28</td>\n      <td>AAPL</td>\n      <td>19</td>\n      <td>76</td>\n      <td>8</td>\n      <td>1206</td>\n      <td>47</td>\n      <td>8909</td>\n      <td>51534</td>\n      <td>327871</td>\n      <td>0.5518</td>\n      <td>0.5299</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2012-06-29</td>\n      <td>AAPL</td>\n      <td>13</td>\n      <td>163</td>\n      <td>9</td>\n      <td>7769</td>\n      <td>16</td>\n      <td>40957</td>\n      <td>15141</td>\n      <td>1576854</td>\n      <td>0.5411</td>\n      <td>0.5888</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2012-06-30</td>\n      <td>AAPL</td>\n      <td>19</td>\n      <td>76</td>\n      <td>8</td>\n      <td>1206</td>\n      <td>47</td>\n      <td>8909</td>\n      <td>51534</td>\n      <td>327871</td>\n      <td>0.5518</td>\n      <td>0.5299</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(jason)\n",
    "df.rename(columns={'symbol':'tic'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:04:27.190776900Z",
     "start_time": "2023-08-23T06:04:27.143874700Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment = df[[\"date\", \"tic\", \"twitterSentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:04:27.867715300Z",
     "start_time": "2023-08-23T06:04:27.837062800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             date   tic  twitterSentiment\n0      2009-01-02  AAPL          0.771962\n1      2009-01-03  AAPL          0.002081\n2      2009-01-04  AAPL          0.580709\n3      2009-01-05  AAPL          0.846955\n4      2009-01-06  AAPL          0.998165\n...           ...   ...               ...\n121737 2020-06-26   WMT          0.705117\n121738 2020-06-27   WMT          0.385721\n121739 2020-06-28   WMT          0.518845\n121740 2020-06-29   WMT          0.450254\n121741 2020-06-30   WMT          0.010505\n\n[121742 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>tic</th>\n      <th>twitterSentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>AAPL</td>\n      <td>0.771962</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-01-03</td>\n      <td>AAPL</td>\n      <td>0.002081</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-01-04</td>\n      <td>AAPL</td>\n      <td>0.580709</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-01-05</td>\n      <td>AAPL</td>\n      <td>0.846955</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-01-06</td>\n      <td>AAPL</td>\n      <td>0.998165</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>121737</th>\n      <td>2020-06-26</td>\n      <td>WMT</td>\n      <td>0.705117</td>\n    </tr>\n    <tr>\n      <th>121738</th>\n      <td>2020-06-27</td>\n      <td>WMT</td>\n      <td>0.385721</td>\n    </tr>\n    <tr>\n      <th>121739</th>\n      <td>2020-06-28</td>\n      <td>WMT</td>\n      <td>0.518845</td>\n    </tr>\n    <tr>\n      <th>121740</th>\n      <td>2020-06-29</td>\n      <td>WMT</td>\n      <td>0.450254</td>\n    </tr>\n    <tr>\n      <th>121741</th>\n      <td>2020-06-30</td>\n      <td>WMT</td>\n      <td>0.010505</td>\n    </tr>\n  </tbody>\n</table>\n<p>121742 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:04:29.924984800Z",
     "start_time": "2023-08-23T06:04:29.876646800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-02 00:00:00\n",
      "2020-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print (train[\"date\"].min())\n",
    "print (train[\"date\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:05:56.872057800Z",
     "start_time": "2023-08-23T06:05:56.840757900Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_generator(ticker, df):\n",
    "    num_samples = len(pd.date_range(df[\"date\"].min(), df[\"date\"].max()))\n",
    "    sampled_sentiment = np.random.random_sample((num_samples,))\n",
    "    random_variable = np.random.random_sample((num_samples,))\n",
    "    sentiment = pd.DataFrame({'date':pd.date_range(df[\"date\"].min(), df[\"date\"].max()),\n",
    "                   'tic':ticker, \n",
    "                   'twitterSentiment':sampled_sentiment,\n",
    "                   'randomVariable':random_variable\n",
    "                   })\n",
    "    return sentiment\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:05:57.221703400Z",
     "start_time": "2023-08-23T06:05:57.189242Z"
    }
   },
   "outputs": [],
   "source": [
    "tickers = train[\"tic\"].unique()\n",
    "#tickers = tickers[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:05:57.573078700Z",
     "start_time": "2023-08-23T06:05:57.526982400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['AAPL', 'AMGN', 'AXP', 'BA', 'CAT', 'CRM', 'CSCO', 'CVX', 'DIS',\n       'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM',\n       'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'V', 'VZ', 'WBA', 'WMT'],\n      dtype=object)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:05:58.399900500Z",
     "start_time": "2023-08-23T06:05:58.232197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             date   tic  twitterSentiment  randomVariable\n0      2009-01-02  AAPL          0.312562        0.328808\n1      2009-01-03  AAPL          0.537763        0.473478\n2      2009-01-04  AAPL          0.999125        0.616726\n3      2009-01-05  AAPL          0.146758        0.694255\n4      2009-01-06  AAPL          0.548800        0.272889\n...           ...   ...               ...             ...\n121737 2020-06-26   WMT          0.527644        0.343278\n121738 2020-06-27   WMT          0.511738        0.169916\n121739 2020-06-28   WMT          0.012103        0.249852\n121740 2020-06-29   WMT          0.843128        0.157613\n121741 2020-06-30   WMT          0.754605        0.414572\n\n[121742 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>tic</th>\n      <th>twitterSentiment</th>\n      <th>randomVariable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>AAPL</td>\n      <td>0.312562</td>\n      <td>0.328808</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-01-03</td>\n      <td>AAPL</td>\n      <td>0.537763</td>\n      <td>0.473478</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-01-04</td>\n      <td>AAPL</td>\n      <td>0.999125</td>\n      <td>0.616726</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-01-05</td>\n      <td>AAPL</td>\n      <td>0.146758</td>\n      <td>0.694255</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-01-06</td>\n      <td>AAPL</td>\n      <td>0.548800</td>\n      <td>0.272889</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>121737</th>\n      <td>2020-06-26</td>\n      <td>WMT</td>\n      <td>0.527644</td>\n      <td>0.343278</td>\n    </tr>\n    <tr>\n      <th>121738</th>\n      <td>2020-06-27</td>\n      <td>WMT</td>\n      <td>0.511738</td>\n      <td>0.169916</td>\n    </tr>\n    <tr>\n      <th>121739</th>\n      <td>2020-06-28</td>\n      <td>WMT</td>\n      <td>0.012103</td>\n      <td>0.249852</td>\n    </tr>\n    <tr>\n      <th>121740</th>\n      <td>2020-06-29</td>\n      <td>WMT</td>\n      <td>0.843128</td>\n      <td>0.157613</td>\n    </tr>\n    <tr>\n      <th>121741</th>\n      <td>2020-06-30</td>\n      <td>WMT</td>\n      <td>0.754605</td>\n      <td>0.414572</td>\n    </tr>\n  </tbody>\n</table>\n<p>121742 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for ticker in tickers:\n",
    "    sentiment = data_generator(ticker, train)\n",
    "    df = pd.concat([df, sentiment], ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "['twitterSentiment', 'randomVariable']"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclude specific columns\n",
    "exclude_columns = ['date', 'tic']\n",
    "SENTIMENT = [col for col in sentiment if col not in exclude_columns]\n",
    "SENTIMENT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "#sentiment = data_generator(\"AAPL\", train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "train['date'] = train['date'].astype('datetime64[ns]')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:05:59.665853700Z",
     "start_time": "2023-08-23T06:05:59.646120100Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:05:59.971105500Z",
     "start_time": "2023-08-23T06:05:59.921089500Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, df,  how='left', left_on=['date','tic'], right_on = ['date','tic'])\n",
    "#train = train.dropna()\n",
    "train.index = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:00.431878400Z",
     "start_time": "2023-08-23T06:06:00.357484500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           date   tic        open        high         low       close  \\\n                                                                        \n0    2009-01-02  AAPL    3.067143    3.251429    3.041429    2.754724   \n0    2009-01-02  AMGN   58.590000   59.080002   57.750000   43.422924   \n0    2009-01-02   AXP   18.570000   19.520000   18.400000   15.256270   \n0    2009-01-02    BA   42.799999   45.560001   42.779999   33.941090   \n0    2009-01-02   CAT   44.910000   46.980000   44.709999   31.254068   \n...         ...   ...         ...         ...         ...         ...   \n2892 2020-06-30   UNH  288.570007  296.450012  287.660004  282.806396   \n2892 2020-06-30     V  191.490005  193.750000  190.160004  189.235092   \n2892 2020-06-30    VZ   54.919998   55.290001   54.360001   46.551456   \n2892 2020-06-30   WBA   42.119999   42.580002   41.759998   37.066734   \n2892 2020-06-30   WMT  119.220001  120.129997  118.540001  114.279716   \n\n           volume  day      macd     boll_ub  ...       dx_30  close_30_sma  \\\n                                              ...                             \n0     746015200.0  4.0  0.000000    2.977272  ...  100.000000      2.754724   \n0       6547900.0  4.0  0.000000    2.977272  ...  100.000000     43.422924   \n0      10955700.0  4.0  0.000000    2.977272  ...  100.000000     15.256270   \n0       7010200.0  4.0  0.000000    2.977272  ...  100.000000     33.941090   \n0       7117200.0  4.0  0.000000    2.977272  ...  100.000000     31.254068   \n...           ...  ...       ...         ...  ...         ...           ...   \n2892    2932900.0  1.0 -0.019138  298.676589  ...    1.846804    283.046084   \n2892    9040100.0  1.0  1.040529  197.185265  ...    2.013358    189.976997   \n2892   17414800.0  1.0 -0.403920   49.824212  ...    8.508886     47.138594   \n2892    4782100.0  1.0 -0.079749   40.460045  ...    1.500723     37.161170   \n2892    6836400.0  1.0 -0.872506  117.578538  ...    3.847271    115.919150   \n\n      close_60_sma        vix  turbulence  twitterSentiment_x  \\\n                                                                \n0         2.754724  39.189999    0.000000            0.771962   \n0        43.422924  39.189999    0.000000            0.449175   \n0        15.256270  39.189999    0.000000            0.713816   \n0        33.941090  39.189999    0.000000            0.855827   \n0        31.254068  39.189999    0.000000            0.696036   \n...            ...        ...         ...                 ...   \n2892    276.148069  30.430000   12.918778            0.233478   \n2892    180.246881  30.430000   12.918778            0.072422   \n2892     47.556786  30.430000   12.918778            0.894316   \n2892     36.971200  30.430000   12.918778            0.210444   \n2892    117.824090  30.430000   12.918778            0.010505   \n\n      twitterSentiment_y  randomVariable_x  twitterSentiment  randomVariable_y  \n                                                                                \n0               0.312562          0.328808          0.312562          0.328808  \n0               0.100892          0.279655          0.100892          0.279655  \n0               0.867858          0.781348          0.867858          0.781348  \n0               0.477656          0.182451          0.477656          0.182451  \n0               0.252966          0.565395          0.252966          0.565395  \n...                  ...               ...               ...               ...  \n2892            0.174850          0.043671          0.174850          0.043671  \n2892            0.478273          0.679544          0.478273          0.679544  \n2892            0.714859          0.143618          0.714859          0.143618  \n2892            0.336928          0.420060          0.336928          0.420060  \n2892            0.754605          0.414572          0.754605          0.414572  \n\n[83897 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>tic</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>day</th>\n      <th>macd</th>\n      <th>boll_ub</th>\n      <th>...</th>\n      <th>dx_30</th>\n      <th>close_30_sma</th>\n      <th>close_60_sma</th>\n      <th>vix</th>\n      <th>turbulence</th>\n      <th>twitterSentiment_x</th>\n      <th>twitterSentiment_y</th>\n      <th>randomVariable_x</th>\n      <th>twitterSentiment</th>\n      <th>randomVariable_y</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>AAPL</td>\n      <td>3.067143</td>\n      <td>3.251429</td>\n      <td>3.041429</td>\n      <td>2.754724</td>\n      <td>746015200.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.977272</td>\n      <td>...</td>\n      <td>100.000000</td>\n      <td>2.754724</td>\n      <td>2.754724</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n      <td>0.771962</td>\n      <td>0.312562</td>\n      <td>0.328808</td>\n      <td>0.312562</td>\n      <td>0.328808</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>AMGN</td>\n      <td>58.590000</td>\n      <td>59.080002</td>\n      <td>57.750000</td>\n      <td>43.422924</td>\n      <td>6547900.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.977272</td>\n      <td>...</td>\n      <td>100.000000</td>\n      <td>43.422924</td>\n      <td>43.422924</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n      <td>0.449175</td>\n      <td>0.100892</td>\n      <td>0.279655</td>\n      <td>0.100892</td>\n      <td>0.279655</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>AXP</td>\n      <td>18.570000</td>\n      <td>19.520000</td>\n      <td>18.400000</td>\n      <td>15.256270</td>\n      <td>10955700.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.977272</td>\n      <td>...</td>\n      <td>100.000000</td>\n      <td>15.256270</td>\n      <td>15.256270</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n      <td>0.713816</td>\n      <td>0.867858</td>\n      <td>0.781348</td>\n      <td>0.867858</td>\n      <td>0.781348</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>BA</td>\n      <td>42.799999</td>\n      <td>45.560001</td>\n      <td>42.779999</td>\n      <td>33.941090</td>\n      <td>7010200.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.977272</td>\n      <td>...</td>\n      <td>100.000000</td>\n      <td>33.941090</td>\n      <td>33.941090</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n      <td>0.855827</td>\n      <td>0.477656</td>\n      <td>0.182451</td>\n      <td>0.477656</td>\n      <td>0.182451</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2009-01-02</td>\n      <td>CAT</td>\n      <td>44.910000</td>\n      <td>46.980000</td>\n      <td>44.709999</td>\n      <td>31.254068</td>\n      <td>7117200.0</td>\n      <td>4.0</td>\n      <td>0.000000</td>\n      <td>2.977272</td>\n      <td>...</td>\n      <td>100.000000</td>\n      <td>31.254068</td>\n      <td>31.254068</td>\n      <td>39.189999</td>\n      <td>0.000000</td>\n      <td>0.696036</td>\n      <td>0.252966</td>\n      <td>0.565395</td>\n      <td>0.252966</td>\n      <td>0.565395</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>2020-06-30</td>\n      <td>UNH</td>\n      <td>288.570007</td>\n      <td>296.450012</td>\n      <td>287.660004</td>\n      <td>282.806396</td>\n      <td>2932900.0</td>\n      <td>1.0</td>\n      <td>-0.019138</td>\n      <td>298.676589</td>\n      <td>...</td>\n      <td>1.846804</td>\n      <td>283.046084</td>\n      <td>276.148069</td>\n      <td>30.430000</td>\n      <td>12.918778</td>\n      <td>0.233478</td>\n      <td>0.174850</td>\n      <td>0.043671</td>\n      <td>0.174850</td>\n      <td>0.043671</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>2020-06-30</td>\n      <td>V</td>\n      <td>191.490005</td>\n      <td>193.750000</td>\n      <td>190.160004</td>\n      <td>189.235092</td>\n      <td>9040100.0</td>\n      <td>1.0</td>\n      <td>1.040529</td>\n      <td>197.185265</td>\n      <td>...</td>\n      <td>2.013358</td>\n      <td>189.976997</td>\n      <td>180.246881</td>\n      <td>30.430000</td>\n      <td>12.918778</td>\n      <td>0.072422</td>\n      <td>0.478273</td>\n      <td>0.679544</td>\n      <td>0.478273</td>\n      <td>0.679544</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>2020-06-30</td>\n      <td>VZ</td>\n      <td>54.919998</td>\n      <td>55.290001</td>\n      <td>54.360001</td>\n      <td>46.551456</td>\n      <td>17414800.0</td>\n      <td>1.0</td>\n      <td>-0.403920</td>\n      <td>49.824212</td>\n      <td>...</td>\n      <td>8.508886</td>\n      <td>47.138594</td>\n      <td>47.556786</td>\n      <td>30.430000</td>\n      <td>12.918778</td>\n      <td>0.894316</td>\n      <td>0.714859</td>\n      <td>0.143618</td>\n      <td>0.714859</td>\n      <td>0.143618</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>2020-06-30</td>\n      <td>WBA</td>\n      <td>42.119999</td>\n      <td>42.580002</td>\n      <td>41.759998</td>\n      <td>37.066734</td>\n      <td>4782100.0</td>\n      <td>1.0</td>\n      <td>-0.079749</td>\n      <td>40.460045</td>\n      <td>...</td>\n      <td>1.500723</td>\n      <td>37.161170</td>\n      <td>36.971200</td>\n      <td>30.430000</td>\n      <td>12.918778</td>\n      <td>0.210444</td>\n      <td>0.336928</td>\n      <td>0.420060</td>\n      <td>0.336928</td>\n      <td>0.420060</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>2020-06-30</td>\n      <td>WMT</td>\n      <td>119.220001</td>\n      <td>120.129997</td>\n      <td>118.540001</td>\n      <td>114.279716</td>\n      <td>6836400.0</td>\n      <td>1.0</td>\n      <td>-0.872506</td>\n      <td>117.578538</td>\n      <td>...</td>\n      <td>3.847271</td>\n      <td>115.919150</td>\n      <td>117.824090</td>\n      <td>30.430000</td>\n      <td>12.918778</td>\n      <td>0.010505</td>\n      <td>0.754605</td>\n      <td>0.414572</td>\n      <td>0.754605</td>\n      <td>0.414572</td>\n    </tr>\n  </tbody>\n</table>\n<p>83897 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:01.972279700Z",
     "start_time": "2023-08-23T06:06:01.956637300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 349\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension +  len(SENTIMENT)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "['twitterSentiment', 'randomVariable']"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTIMENT"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:02.969957700Z",
     "start_time": "2023-08-23T06:06:02.953981700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "['macd',\n 'boll_ub',\n 'boll_lb',\n 'rsi_30',\n 'cci_30',\n 'dx_30',\n 'close_30_sma',\n 'close_60_sma']"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDICATORS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:03.788309Z",
     "start_time": "2023-08-23T06:06:03.763707600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WsOLoeNcJF8Q",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:04.332586Z",
     "start_time": "2023-08-23T06:06:04.283590200Z"
    }
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"sentiment_list\" : SENTIMENT,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:05.541279300Z",
     "start_time": "2023-08-23T06:06:05.494015200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "364PsqckttcQ",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:11.154422Z",
     "start_time": "2023-08-23T06:06:11.123593400Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:14.020357900Z",
     "start_time": "2023-08-23T06:06:13.958069700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81",
    "ExecuteTime": {
     "end_time": "2023-08-23T06:06:32.365749900Z",
     "start_time": "2023-08-23T06:06:15.670698500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 221         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -7.68       |\n",
      "|    reward             | 0.113246076 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.144       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 238        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.154     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -15.4      |\n",
      "|    reward             | -2.1484027 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.323      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 245      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | -0.107   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -248     |\n",
      "|    reward             | 4.274392 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 35.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 249       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 9.63      |\n",
      "|    reward             | 0.7137551 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.77      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 252        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 371        |\n",
      "|    reward             | -11.898218 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 111        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 253       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -1.8      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 221       |\n",
      "|    reward             | 0.6549009 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 32.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 255        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -0.289     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -122       |\n",
      "|    reward             | -3.8509636 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 11.7       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 254         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 113         |\n",
      "|    reward             | -0.21699661 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 13.1        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 255         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 178         |\n",
      "|    reward             | -0.32903376 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 22.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 256       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -112      |\n",
      "|    reward             | -6.709401 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 10.4      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 257      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -228     |\n",
      "|    reward             | 3.498876 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 35.6     |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 257          |\n",
      "|    iterations         | 1200         |\n",
      "|    time_elapsed       | 23           |\n",
      "|    total_timesteps    | 6000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.3        |\n",
      "|    explained_variance | -0.0102      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1199         |\n",
      "|    policy_loss        | -133         |\n",
      "|    reward             | -0.016963897 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 13.4         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -24.1      |\n",
      "|    reward             | -4.6160307 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.51       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 32.8      |\n",
      "|    reward             | 2.148205  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.26      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 178       |\n",
      "|    reward             | 2.7761128 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 18.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 257       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 4.94      |\n",
      "|    reward             | 5.6398754 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.44      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 256       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -236      |\n",
      "|    reward             | 3.8260918 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 44.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 256       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -32.9     |\n",
      "|    reward             | 1.0421152 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.847     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 256        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -0.0953    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 180        |\n",
      "|    reward             | -0.4304108 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 21.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 255        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0.234      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -115       |\n",
      "|    reward             | 0.59794873 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.29       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 255       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 87.3      |\n",
      "|    reward             | 3.3856366 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 16.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 255        |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | -50.3      |\n",
      "|    reward             | -14.722383 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 30.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 255       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -3.41e+03 |\n",
      "|    reward             | 14.221941 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.58e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 255       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 105       |\n",
      "|    reward             | 0.6835316 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 13.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 256      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | -24.1    |\n",
      "|    reward             | 2.065639 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.537    |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 256         |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | -1.13       |\n",
      "|    reward             | -0.26957443 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.03        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 256       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | -6.5      |\n",
      "|    reward             | 3.0820591 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.107     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 256       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0.294     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 191       |\n",
      "|    reward             | 1.3417168 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 23        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 256       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -54.2     |\n",
      "|    reward             | 1.2286404 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.82      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 257       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | -5.21     |\n",
      "|    reward             | 0.8975458 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.859     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 257        |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | 84.4       |\n",
      "|    reward             | -0.6173855 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 5.17       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 257        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 62         |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | 43.3       |\n",
      "|    reward             | -1.3679014 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 9.13       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 258         |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 63          |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | 243         |\n",
      "|    reward             | -0.38338944 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 36.9        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 257       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 100       |\n",
      "|    reward             | 4.99516   |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 21.2      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 258         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 67          |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0.104       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 157         |\n",
      "|    reward             | -0.62059593 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 15.5        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 257      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -86.5    |\n",
      "|    reward             | 1.896004 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 8.08     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 257      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 71       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 96.5     |\n",
      "|    reward             | 2.428928 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 7.35     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 257       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -0.171    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 5.41      |\n",
      "|    reward             | 1.3460364 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.94      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 257      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | -203     |\n",
      "|    reward             | 2.04022  |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 27       |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 77        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -159      |\n",
      "|    reward             | 4.423502  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 25.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 258         |\n",
      "|    iterations         | 4100        |\n",
      "|    time_elapsed       | 79          |\n",
      "|    total_timesteps    | 20500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4099        |\n",
      "|    policy_loss        | -18.1       |\n",
      "|    reward             | 0.099361464 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.882       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 81         |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | -144       |\n",
      "|    reward             | -1.5004879 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 13.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -29.1     |\n",
      "|    reward             | 4.995676  |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.3       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | 58.6      |\n",
      "|    reward             | 3.5520535 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 8.12      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 87        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 299       |\n",
      "|    reward             | 3.1050217 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 60        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 89         |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 191        |\n",
      "|    reward             | 0.89258224 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 27         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | -0.0103    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -100       |\n",
      "|    reward             | 0.32363772 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 9.62       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 92         |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -218       |\n",
      "|    reward             | -1.0852952 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 32.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -47.3     |\n",
      "|    reward             | 2.7104726 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 4.87      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -26       |\n",
      "|    reward             | 2.2747552 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 7.62      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 258      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 98       |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 653      |\n",
      "|    reward             | -1.04141 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 288      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 11.2      |\n",
      "|    reward             | 13.830259 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 301       |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9001105.26\n",
      "total_reward: 8001105.26\n",
      "total_cost: 27728.63\n",
      "total_trades: 50472\n",
      "Sharpe: 1.079\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 258      |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | -0.0779  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | -54      |\n",
      "|    reward             | 0.403503 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.69     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 104        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -125       |\n",
      "|    reward             | 0.30487776 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 13.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 106       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | 124       |\n",
      "|    reward             | 2.5679984 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 13.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -159      |\n",
      "|    reward             | 3.0552678 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 21.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 259      |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -775     |\n",
      "|    reward             | -9.08334 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 467      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 259       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | -16.4     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 345       |\n",
      "|    reward             | 2.0034108 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 116       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 259         |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 113         |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | 51.6        |\n",
      "|    reward             | -0.47343618 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 2.59        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 115        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 356        |\n",
      "|    reward             | -1.1324562 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 92.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 117       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | -86.5     |\n",
      "|    reward             | -5.986976 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 12.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | 0.883     |\n",
      "|    reward             | 1.2630218 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.25      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 1.07e+03  |\n",
      "|    reward             | 16.663269 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.03e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | -1.6      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 85.8      |\n",
      "|    reward             | 1.9242967 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.19      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 257       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 5.57      |\n",
      "|    reward             | -3.592099 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 4.14      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 257        |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 127        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -53.5      |\n",
      "|    reward             | -0.5582843 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 6.56       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 257        |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 129        |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | -1.62e+03  |\n",
      "|    reward             | -16.882399 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.96e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -355      |\n",
      "|    reward             | 2.1105247 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 91.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | -427      |\n",
      "|    reward             | -6.262049 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 262       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 28.1      |\n",
      "|    reward             | 0.1571427 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.691     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 7100       |\n",
      "|    time_elapsed       | 137        |\n",
      "|    total_timesteps    | 35500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7099       |\n",
      "|    policy_loss        | 41.7       |\n",
      "|    reward             | 0.33376986 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.99       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 139        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -272       |\n",
      "|    reward             | 0.29656276 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 45.3       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 259          |\n",
      "|    iterations         | 7300         |\n",
      "|    time_elapsed       | 140          |\n",
      "|    total_timesteps    | 36500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7299         |\n",
      "|    policy_loss        | -148         |\n",
      "|    reward             | -0.010756248 |\n",
      "|    std                | 1.04         |\n",
      "|    value_loss         | 19.2         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 259        |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 142        |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | 232        |\n",
      "|    reward             | -6.4345055 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 62.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 259       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 144       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 424       |\n",
      "|    reward             | -8.146994 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 121       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 259       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 146       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | -189      |\n",
      "|    reward             | 1.6216983 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 20.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 259        |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 148        |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | -15.8      |\n",
      "|    reward             | 0.25216016 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.23       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 259          |\n",
      "|    iterations         | 7800         |\n",
      "|    time_elapsed       | 150          |\n",
      "|    total_timesteps    | 39000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7799         |\n",
      "|    policy_loss        | 57.9         |\n",
      "|    reward             | -0.031232065 |\n",
      "|    std                | 1.04         |\n",
      "|    value_loss         | 4.06         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | 60.5      |\n",
      "|    reward             | 3.7598176 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 16        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 154        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | 9.85       |\n",
      "|    reward             | -2.6965744 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.14       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | 761       |\n",
      "|    reward             | 12.260159 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 299       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 158        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -150       |\n",
      "|    reward             | 0.39594945 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 12.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 160        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -46.8      |\n",
      "|    reward             | -2.6202598 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 162        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -123       |\n",
      "|    reward             | -1.5270183 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 9.5        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 8500       |\n",
      "|    time_elapsed       | 164        |\n",
      "|    total_timesteps    | 42500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8499       |\n",
      "|    policy_loss        | -242       |\n",
      "|    reward             | 0.47393987 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 32.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 166        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 99.4       |\n",
      "|    reward             | -25.256287 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 50.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 168       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -0.0159   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | 14.9      |\n",
      "|    reward             | 1.6749034 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.49      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 170        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -0.000355  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | -70.4      |\n",
      "|    reward             | 0.70812774 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 3.11       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 172       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 112       |\n",
      "|    reward             | 2.284058  |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 9.28      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 174        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 44.7       |\n",
      "|    reward             | -2.7512279 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 17.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 176       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -37.4     |\n",
      "|    reward             | 1.8274072 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.37      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 177       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | 36.1      |\n",
      "|    reward             | 5.0680265 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 6.7       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 179       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0.00745   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -202      |\n",
      "|    reward             | 1.4597983 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 26.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 181        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | 45.1       |\n",
      "|    reward             | -1.2292585 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.96       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 258         |\n",
      "|    iterations         | 9500        |\n",
      "|    time_elapsed       | 183         |\n",
      "|    total_timesteps    | 47500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9499        |\n",
      "|    policy_loss        | 185         |\n",
      "|    reward             | -0.08556621 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 22.6        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 185        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -636       |\n",
      "|    reward             | -1.8868514 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 222        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 187        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | -35.2      |\n",
      "|    reward             | -1.4068207 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 3.33       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 258       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 189       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 209       |\n",
      "|    reward             | 6.6874228 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 59        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 258        |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 191        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | 19.7       |\n",
      "|    reward             | 0.05296647 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.502      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 259         |\n",
      "|    iterations         | 10000       |\n",
      "|    time_elapsed       | 193         |\n",
      "|    total_timesteps    | 50000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9999        |\n",
      "|    policy_loss        | 58.4        |\n",
      "|    reward             | -0.12533137 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 3.03        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 259       |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 194       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | -164      |\n",
      "|    reward             | 2.6731153 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 17.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 259        |\n",
      "|    iterations         | 10200      |\n",
      "|    time_elapsed       | 196        |\n",
      "|    total_timesteps    | 51000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10199      |\n",
      "|    policy_loss        | 80.9       |\n",
      "|    reward             | -0.9310504 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 4.07       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 259        |\n",
      "|    iterations         | 10300      |\n",
      "|    time_elapsed       | 198        |\n",
      "|    total_timesteps    | 51500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10299      |\n",
      "|    policy_loss        | -22        |\n",
      "|    reward             | -2.7059283 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 13.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 259       |\n",
      "|    iterations         | 10400     |\n",
      "|    time_elapsed       | 200       |\n",
      "|    total_timesteps    | 52000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10399     |\n",
      "|    policy_loss        | 243       |\n",
      "|    reward             | 61.685764 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 219       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 259        |\n",
      "|    iterations         | 10500      |\n",
      "|    time_elapsed       | 202        |\n",
      "|    total_timesteps    | 52500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10499      |\n",
      "|    policy_loss        | 158        |\n",
      "|    reward             | 0.86972123 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 15.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 259        |\n",
      "|    iterations         | 10600      |\n",
      "|    time_elapsed       | 204        |\n",
      "|    total_timesteps    | 53000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10599      |\n",
      "|    policy_loss        | -44.2      |\n",
      "|    reward             | 0.43728575 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.82       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 259        |\n",
      "|    iterations         | 10700      |\n",
      "|    time_elapsed       | 205        |\n",
      "|    total_timesteps    | 53500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10699      |\n",
      "|    policy_loss        | 127        |\n",
      "|    reward             | 0.96694005 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 9.12       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 259       |\n",
      "|    iterations         | 10800     |\n",
      "|    time_elapsed       | 207       |\n",
      "|    total_timesteps    | 54000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10799     |\n",
      "|    policy_loss        | -89.7     |\n",
      "|    reward             | 1.3036672 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 8         |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 259         |\n",
      "|    iterations         | 10900       |\n",
      "|    time_elapsed       | 209         |\n",
      "|    total_timesteps    | 54500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10899       |\n",
      "|    policy_loss        | 10.6        |\n",
      "|    reward             | -0.52544254 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 3.06        |\n",
      "---------------------------------------\n",
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5399272.40\n",
      "total_reward: 4399272.40\n",
      "total_cost: 11750.76\n",
      "total_trades: 45956\n",
      "Sharpe: 0.890\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 259        |\n",
      "|    iterations         | 11000      |\n",
      "|    time_elapsed       | 211        |\n",
      "|    total_timesteps    | 55000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10999      |\n",
      "|    policy_loss        | -25.1      |\n",
      "|    reward             | -0.5944906 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.969      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 259         |\n",
      "|    iterations         | 11100       |\n",
      "|    time_elapsed       | 213         |\n",
      "|    total_timesteps    | 55500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11099       |\n",
      "|    policy_loss        | 7.42        |\n",
      "|    reward             | -0.85499847 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.248       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 259       |\n",
      "|    iterations         | 11200     |\n",
      "|    time_elapsed       | 215       |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11199     |\n",
      "|    policy_loss        | -50.6     |\n",
      "|    reward             | -1.557381 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.51      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 260      |\n",
      "|    iterations         | 11300    |\n",
      "|    time_elapsed       | 217      |\n",
      "|    total_timesteps    | 56500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 0.0451   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11299    |\n",
      "|    policy_loss        | -246     |\n",
      "|    reward             | 4.120591 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 55.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 260      |\n",
      "|    iterations         | 11400    |\n",
      "|    time_elapsed       | 219      |\n",
      "|    total_timesteps    | 57000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11399    |\n",
      "|    policy_loss        | -230     |\n",
      "|    reward             | 3.885846 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 33.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 260        |\n",
      "|    iterations         | 11500      |\n",
      "|    time_elapsed       | 220        |\n",
      "|    total_timesteps    | 57500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11499      |\n",
      "|    policy_loss        | 498        |\n",
      "|    reward             | -13.998222 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 142        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 11600     |\n",
      "|    time_elapsed       | 222       |\n",
      "|    total_timesteps    | 58000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -0.00955  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11599     |\n",
      "|    policy_loss        | 133       |\n",
      "|    reward             | 1.2221235 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 15.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 11700     |\n",
      "|    time_elapsed       | 224       |\n",
      "|    total_timesteps    | 58500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -0.186    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | -171      |\n",
      "|    reward             | 1.9536979 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 18        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 226       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | 169       |\n",
      "|    reward             | -1.284029 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 19.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 11900     |\n",
      "|    time_elapsed       | 228       |\n",
      "|    total_timesteps    | 59500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | -0.0193   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11899     |\n",
      "|    policy_loss        | -166      |\n",
      "|    reward             | 5.2974105 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 41.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 12000     |\n",
      "|    time_elapsed       | 229       |\n",
      "|    total_timesteps    | 60000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11999     |\n",
      "|    policy_loss        | -92.9     |\n",
      "|    reward             | 4.20489   |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 24.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 260       |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 231       |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0.0309    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | 304       |\n",
      "|    reward             | 0.3851193 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 109       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 12200     |\n",
      "|    time_elapsed       | 233       |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | -142      |\n",
      "|    reward             | 0.5062644 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 12.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 261        |\n",
      "|    iterations         | 12300      |\n",
      "|    time_elapsed       | 235        |\n",
      "|    total_timesteps    | 61500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12299      |\n",
      "|    policy_loss        | -118       |\n",
      "|    reward             | -0.9023242 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 8.77       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 261        |\n",
      "|    iterations         | 12400      |\n",
      "|    time_elapsed       | 237        |\n",
      "|    total_timesteps    | 62000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12399      |\n",
      "|    policy_loss        | 30.2       |\n",
      "|    reward             | 0.46816334 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.44       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 12500     |\n",
      "|    time_elapsed       | 239       |\n",
      "|    total_timesteps    | 62500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12499     |\n",
      "|    policy_loss        | -235      |\n",
      "|    reward             | 1.8840423 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 35.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 261        |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 240        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | 79.5       |\n",
      "|    reward             | 0.07370225 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 3.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 261        |\n",
      "|    iterations         | 12700      |\n",
      "|    time_elapsed       | 242        |\n",
      "|    total_timesteps    | 63500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12699      |\n",
      "|    policy_loss        | 86         |\n",
      "|    reward             | -1.6250122 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 5.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 261        |\n",
      "|    iterations         | 12800      |\n",
      "|    time_elapsed       | 244        |\n",
      "|    total_timesteps    | 64000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12799      |\n",
      "|    policy_loss        | -192       |\n",
      "|    reward             | -0.9794809 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 29.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 12900     |\n",
      "|    time_elapsed       | 246       |\n",
      "|    total_timesteps    | 64500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12899     |\n",
      "|    policy_loss        | -58.8     |\n",
      "|    reward             | 1.1096468 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 2.82      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 13000     |\n",
      "|    time_elapsed       | 248       |\n",
      "|    total_timesteps    | 65000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | 2.21      |\n",
      "|    reward             | 1.7209483 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.463     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 261      |\n",
      "|    iterations         | 13100    |\n",
      "|    time_elapsed       | 250      |\n",
      "|    total_timesteps    | 65500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | 22.9     |\n",
      "|    reward             | 2.765324 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 3.5      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 261      |\n",
      "|    iterations         | 13200    |\n",
      "|    time_elapsed       | 251      |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | -254     |\n",
      "|    reward             | 2.221102 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 47.7     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 253       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -293      |\n",
      "|    reward             | -4.352138 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 50        |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 262      |\n",
      "|    iterations         | 13400    |\n",
      "|    time_elapsed       | 255      |\n",
      "|    total_timesteps    | 67000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13399    |\n",
      "|    policy_loss        | 16.4     |\n",
      "|    reward             | 0.510847 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 0.613    |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 262         |\n",
      "|    iterations         | 13500       |\n",
      "|    time_elapsed       | 257         |\n",
      "|    total_timesteps    | 67500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13499       |\n",
      "|    policy_loss        | 160         |\n",
      "|    reward             | -0.24174355 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 15.6        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 259        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -345       |\n",
      "|    reward             | -0.4974162 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 74         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 262         |\n",
      "|    iterations         | 13700       |\n",
      "|    time_elapsed       | 261         |\n",
      "|    total_timesteps    | 68500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13699       |\n",
      "|    policy_loss        | -223        |\n",
      "|    reward             | -0.27956024 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 34.6        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 262       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | -0.234    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | -27.8     |\n",
      "|    reward             | -8.739026 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 3.58      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 13900     |\n",
      "|    time_elapsed       | 264       |\n",
      "|    total_timesteps    | 69500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | -48.1     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13899     |\n",
      "|    policy_loss        | 1.16e+03  |\n",
      "|    reward             | 0.2757975 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 795       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 266        |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | 24.3       |\n",
      "|    reward             | 0.04620764 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.656      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 268        |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | 216        |\n",
      "|    reward             | -2.2172184 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 32.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 14200     |\n",
      "|    time_elapsed       | 270       |\n",
      "|    total_timesteps    | 71000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14199     |\n",
      "|    policy_loss        | 276       |\n",
      "|    reward             | 2.2503226 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 61.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 272       |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | 76.2      |\n",
      "|    reward             | 0.8369808 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 3.83      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 14400     |\n",
      "|    time_elapsed       | 274       |\n",
      "|    total_timesteps    | 72000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14399     |\n",
      "|    policy_loss        | -56.6     |\n",
      "|    reward             | 1.4718001 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 18        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 261        |\n",
      "|    iterations         | 14500      |\n",
      "|    time_elapsed       | 276        |\n",
      "|    total_timesteps    | 72500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0.000809   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14499      |\n",
      "|    policy_loss        | -9.63      |\n",
      "|    reward             | 0.06970519 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.36       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 261      |\n",
      "|    iterations         | 14600    |\n",
      "|    time_elapsed       | 278      |\n",
      "|    total_timesteps    | 73000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14599    |\n",
      "|    policy_loss        | 108      |\n",
      "|    reward             | 3.671005 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 8.46     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 261       |\n",
      "|    iterations         | 14700     |\n",
      "|    time_elapsed       | 280       |\n",
      "|    total_timesteps    | 73500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14699     |\n",
      "|    policy_loss        | -131      |\n",
      "|    reward             | 2.4404104 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 10        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 282        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | -884       |\n",
      "|    reward             | -1.2960128 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 416        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 14900      |\n",
      "|    time_elapsed       | 284        |\n",
      "|    total_timesteps    | 74500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14899      |\n",
      "|    policy_loss        | 48.4       |\n",
      "|    reward             | -0.7300328 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 4.46       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 286       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | 56.5      |\n",
      "|    reward             | -6.516063 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 21.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 15100     |\n",
      "|    time_elapsed       | 287       |\n",
      "|    total_timesteps    | 75500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15099     |\n",
      "|    policy_loss        | -31.4     |\n",
      "|    reward             | 0.9514219 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 8.1       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 289       |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | 2.92      |\n",
      "|    reward             | 0.8426457 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 0.535     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 15300     |\n",
      "|    time_elapsed       | 291       |\n",
      "|    total_timesteps    | 76500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -0.000593 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15299     |\n",
      "|    policy_loss        | -124      |\n",
      "|    reward             | 2.2071195 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 13.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 15400      |\n",
      "|    time_elapsed       | 293        |\n",
      "|    total_timesteps    | 77000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15399      |\n",
      "|    policy_loss        | 203        |\n",
      "|    reward             | -4.7883997 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 24.8       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 262      |\n",
      "|    iterations         | 15500    |\n",
      "|    time_elapsed       | 295      |\n",
      "|    total_timesteps    | 77500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | 306      |\n",
      "|    reward             | 6.349119 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 63.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 15600     |\n",
      "|    time_elapsed       | 297       |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15599     |\n",
      "|    policy_loss        | -161      |\n",
      "|    reward             | 1.4430661 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 18.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 15700      |\n",
      "|    time_elapsed       | 299        |\n",
      "|    total_timesteps    | 78500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15699      |\n",
      "|    policy_loss        | 83         |\n",
      "|    reward             | -1.2658695 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 5.33       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 300       |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | 78.3      |\n",
      "|    reward             | 2.9399073 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 8.13      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 15900      |\n",
      "|    time_elapsed       | 302        |\n",
      "|    total_timesteps    | 79500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15899      |\n",
      "|    policy_loss        | 40.3       |\n",
      "|    reward             | 0.23435871 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.1        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 16000      |\n",
      "|    time_elapsed       | 304        |\n",
      "|    total_timesteps    | 80000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15999      |\n",
      "|    policy_loss        | -222       |\n",
      "|    reward             | 0.23882662 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 40.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 16100     |\n",
      "|    time_elapsed       | 306       |\n",
      "|    total_timesteps    | 80500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.3     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | -58.5     |\n",
      "|    reward             | 0.2927397 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 13.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 262         |\n",
      "|    iterations         | 16200       |\n",
      "|    time_elapsed       | 308         |\n",
      "|    total_timesteps    | 81000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16199       |\n",
      "|    policy_loss        | 59.1        |\n",
      "|    reward             | -0.24153998 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 25.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 310        |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | 0.982      |\n",
      "|    reward             | 0.09168793 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.608      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 312       |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | 72.2      |\n",
      "|    reward             | 1.5234405 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 3.16      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 16500     |\n",
      "|    time_elapsed       | 314       |\n",
      "|    total_timesteps    | 82500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16499     |\n",
      "|    policy_loss        | -78       |\n",
      "|    reward             | 4.339094  |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 5.48      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 16600     |\n",
      "|    time_elapsed       | 315       |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | 123       |\n",
      "|    reward             | 0.5929114 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 10.7      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 262      |\n",
      "|    iterations         | 16700    |\n",
      "|    time_elapsed       | 317      |\n",
      "|    total_timesteps    | 83500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16699    |\n",
      "|    policy_loss        | -640     |\n",
      "|    reward             | 4.924917 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 329      |\n",
      "------------------------------------\n",
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4320193.98\n",
      "total_reward: 3320193.98\n",
      "total_cost: 8188.20\n",
      "total_trades: 40092\n",
      "Sharpe: 0.793\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 319       |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.6     |\n",
      "|    explained_variance | 0.041     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | 20.6      |\n",
      "|    reward             | 0.3001544 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 2.67      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 321       |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | -68.2     |\n",
      "|    reward             | 0.1286881 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 3.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 17000      |\n",
      "|    time_elapsed       | 323        |\n",
      "|    total_timesteps    | 85000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16999      |\n",
      "|    policy_loss        | 133        |\n",
      "|    reward             | -0.9108236 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 9.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 325       |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | 35.5      |\n",
      "|    reward             | 1.3050231 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 4.36      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 17200      |\n",
      "|    time_elapsed       | 327        |\n",
      "|    total_timesteps    | 86000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17199      |\n",
      "|    policy_loss        | 124        |\n",
      "|    reward             | -1.0388356 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 7.61       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 17300      |\n",
      "|    time_elapsed       | 328        |\n",
      "|    total_timesteps    | 86500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17299      |\n",
      "|    policy_loss        | -389       |\n",
      "|    reward             | -2.4467177 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 132        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 330        |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.8      |\n",
      "|    explained_variance | -0.0024    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | -180       |\n",
      "|    reward             | 0.89906234 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 21.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 332       |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.9     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | 53.4      |\n",
      "|    reward             | 2.1623387 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 2.67      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 17600     |\n",
      "|    time_elapsed       | 334       |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.9     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17599     |\n",
      "|    policy_loss        | 133       |\n",
      "|    reward             | 1.1793549 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 14.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 17700      |\n",
      "|    time_elapsed       | 336        |\n",
      "|    total_timesteps    | 88500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17699      |\n",
      "|    policy_loss        | 137        |\n",
      "|    reward             | 0.15011612 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 11.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 17800     |\n",
      "|    time_elapsed       | 338       |\n",
      "|    total_timesteps    | 89000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17799     |\n",
      "|    policy_loss        | 144       |\n",
      "|    reward             | 1.2280915 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 12.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 17900     |\n",
      "|    time_elapsed       | 339       |\n",
      "|    total_timesteps    | 89500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | -626      |\n",
      "|    reward             | 3.6549244 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 198       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 18000       |\n",
      "|    time_elapsed       | 341         |\n",
      "|    total_timesteps    | 90000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17999       |\n",
      "|    policy_loss        | -15.3       |\n",
      "|    reward             | -0.00783226 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.453       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 18100      |\n",
      "|    time_elapsed       | 343        |\n",
      "|    total_timesteps    | 90500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18099      |\n",
      "|    policy_loss        | -70.5      |\n",
      "|    reward             | 0.97197556 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 3.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 18200     |\n",
      "|    time_elapsed       | 345       |\n",
      "|    total_timesteps    | 91000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18199     |\n",
      "|    policy_loss        | 64.2      |\n",
      "|    reward             | 1.0478344 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 3.66      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 18300     |\n",
      "|    time_elapsed       | 347       |\n",
      "|    total_timesteps    | 91500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18299     |\n",
      "|    policy_loss        | 128       |\n",
      "|    reward             | -2.227428 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 8.65      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 349       |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | -141      |\n",
      "|    reward             | -5.306576 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 20        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 18500     |\n",
      "|    time_elapsed       | 351       |\n",
      "|    total_timesteps    | 92500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18499     |\n",
      "|    policy_loss        | -68.2     |\n",
      "|    reward             | 35.881367 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 115       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 18600       |\n",
      "|    time_elapsed       | 353         |\n",
      "|    total_timesteps    | 93000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18599       |\n",
      "|    policy_loss        | 32.2        |\n",
      "|    reward             | -0.47466707 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 2.26        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 263      |\n",
      "|    iterations         | 18700    |\n",
      "|    time_elapsed       | 355      |\n",
      "|    total_timesteps    | 93500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18699    |\n",
      "|    policy_loss        | -54.2    |\n",
      "|    reward             | 1.18395  |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 4        |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 18800      |\n",
      "|    time_elapsed       | 357        |\n",
      "|    total_timesteps    | 94000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18799      |\n",
      "|    policy_loss        | 131        |\n",
      "|    reward             | -0.8417781 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 11.6       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 18900       |\n",
      "|    time_elapsed       | 359         |\n",
      "|    total_timesteps    | 94500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18899       |\n",
      "|    policy_loss        | -28.3       |\n",
      "|    reward             | -0.47271803 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 0.673       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 262         |\n",
      "|    iterations         | 19000       |\n",
      "|    time_elapsed       | 361         |\n",
      "|    total_timesteps    | 95000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18999       |\n",
      "|    policy_loss        | -279        |\n",
      "|    reward             | -0.44025868 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 49.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 19100      |\n",
      "|    time_elapsed       | 363        |\n",
      "|    total_timesteps    | 95500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19099      |\n",
      "|    policy_loss        | -38.7      |\n",
      "|    reward             | 0.42355832 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 19200      |\n",
      "|    time_elapsed       | 365        |\n",
      "|    total_timesteps    | 96000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19199      |\n",
      "|    policy_loss        | -36.2      |\n",
      "|    reward             | -1.6770352 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 2.48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 367        |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | -20.5      |\n",
      "|    reward             | 0.18100819 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.415      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 262      |\n",
      "|    iterations         | 19400    |\n",
      "|    time_elapsed       | 369      |\n",
      "|    total_timesteps    | 97000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19399    |\n",
      "|    policy_loss        | -87.2    |\n",
      "|    reward             | 2.083217 |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 10.8     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 371        |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | -39.5      |\n",
      "|    reward             | -2.2274303 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 1.16       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 262         |\n",
      "|    iterations         | 19600       |\n",
      "|    time_elapsed       | 372         |\n",
      "|    total_timesteps    | 98000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19599       |\n",
      "|    policy_loss        | 121         |\n",
      "|    reward             | -0.64872754 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 11.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 19700      |\n",
      "|    time_elapsed       | 374        |\n",
      "|    total_timesteps    | 98500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | 0.0196     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19699      |\n",
      "|    policy_loss        | 94.1       |\n",
      "|    reward             | 0.14664915 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 5.07       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 19800      |\n",
      "|    time_elapsed       | 376        |\n",
      "|    total_timesteps    | 99000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19799      |\n",
      "|    policy_loss        | 18.1       |\n",
      "|    reward             | 0.14614011 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 1.76       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 19900      |\n",
      "|    time_elapsed       | 378        |\n",
      "|    total_timesteps    | 99500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19899      |\n",
      "|    policy_loss        | 84.9       |\n",
      "|    reward             | -2.1093051 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 4.97       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 20000      |\n",
      "|    time_elapsed       | 380        |\n",
      "|    total_timesteps    | 100000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19999      |\n",
      "|    policy_loss        | -222       |\n",
      "|    reward             | -3.8628254 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 33.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 20100     |\n",
      "|    time_elapsed       | 382       |\n",
      "|    total_timesteps    | 100500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20099     |\n",
      "|    policy_loss        | 23.2      |\n",
      "|    reward             | 0.0568702 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 1.47      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 262      |\n",
      "|    iterations         | 20200    |\n",
      "|    time_elapsed       | 384      |\n",
      "|    total_timesteps    | 101000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20199    |\n",
      "|    policy_loss        | 579      |\n",
      "|    reward             | -5.18182 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 222      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 20300      |\n",
      "|    time_elapsed       | 386        |\n",
      "|    total_timesteps    | 101500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 20299      |\n",
      "|    policy_loss        | 34.8       |\n",
      "|    reward             | 0.53220797 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 1.47       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 20400     |\n",
      "|    time_elapsed       | 387       |\n",
      "|    total_timesteps    | 102000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20399     |\n",
      "|    policy_loss        | -91.7     |\n",
      "|    reward             | 0.9731874 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 4.62      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 262      |\n",
      "|    iterations         | 20500    |\n",
      "|    time_elapsed       | 389      |\n",
      "|    total_timesteps    | 102500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 20499    |\n",
      "|    policy_loss        | -154     |\n",
      "|    reward             | -1.05224 |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 17.2     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 20600      |\n",
      "|    time_elapsed       | 391        |\n",
      "|    total_timesteps    | 103000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 20599      |\n",
      "|    policy_loss        | -66.5      |\n",
      "|    reward             | -3.6066816 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 5.52       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 20700      |\n",
      "|    time_elapsed       | 393        |\n",
      "|    total_timesteps    | 103500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 20699      |\n",
      "|    policy_loss        | -34.3      |\n",
      "|    reward             | -3.6065838 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 3.93       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 262        |\n",
      "|    iterations         | 20800      |\n",
      "|    time_elapsed       | 395        |\n",
      "|    total_timesteps    | 104000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 20799      |\n",
      "|    policy_loss        | 5.28       |\n",
      "|    reward             | -3.3927314 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 4.19       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 20900     |\n",
      "|    time_elapsed       | 397       |\n",
      "|    total_timesteps    | 104500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.5     |\n",
      "|    explained_variance | 0.0338    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20899     |\n",
      "|    policy_loss        | -322      |\n",
      "|    reward             | 3.0445278 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 59.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 262       |\n",
      "|    iterations         | 21000     |\n",
      "|    time_elapsed       | 399       |\n",
      "|    total_timesteps    | 105000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20999     |\n",
      "|    policy_loss        | -99.9     |\n",
      "|    reward             | 2.9476888 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 5.39      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 21100      |\n",
      "|    time_elapsed       | 401        |\n",
      "|    total_timesteps    | 105500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 21099      |\n",
      "|    policy_loss        | 129        |\n",
      "|    reward             | 0.22115298 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 10.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 21200     |\n",
      "|    time_elapsed       | 402       |\n",
      "|    total_timesteps    | 106000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.5     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21199     |\n",
      "|    policy_loss        | -78.8     |\n",
      "|    reward             | 1.5152456 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 6.69      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 21300       |\n",
      "|    time_elapsed       | 404         |\n",
      "|    total_timesteps    | 106500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 21299       |\n",
      "|    policy_loss        | -270        |\n",
      "|    reward             | -0.83156675 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 46.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 21400     |\n",
      "|    time_elapsed       | 406       |\n",
      "|    total_timesteps    | 107000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.6     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21399     |\n",
      "|    policy_loss        | 31.4      |\n",
      "|    reward             | 1.3096164 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 21500      |\n",
      "|    time_elapsed       | 408        |\n",
      "|    total_timesteps    | 107500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 21499      |\n",
      "|    policy_loss        | 52.4       |\n",
      "|    reward             | -1.6576706 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 1.98       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 21600       |\n",
      "|    time_elapsed       | 410         |\n",
      "|    total_timesteps    | 108000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 21599       |\n",
      "|    policy_loss        | 109         |\n",
      "|    reward             | -0.39935106 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 6.99        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 21700     |\n",
      "|    time_elapsed       | 412       |\n",
      "|    total_timesteps    | 108500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21699     |\n",
      "|    policy_loss        | -263      |\n",
      "|    reward             | 2.5686424 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 33.6      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 263      |\n",
      "|    iterations         | 21800    |\n",
      "|    time_elapsed       | 414      |\n",
      "|    total_timesteps    | 109000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 21799    |\n",
      "|    policy_loss        | -212     |\n",
      "|    reward             | 1.399019 |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 24.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 21900     |\n",
      "|    time_elapsed       | 415       |\n",
      "|    total_timesteps    | 109500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21899     |\n",
      "|    policy_loss        | 537       |\n",
      "|    reward             | 3.1694536 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 137       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 22000     |\n",
      "|    time_elapsed       | 417       |\n",
      "|    total_timesteps    | 110000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21999     |\n",
      "|    policy_loss        | 65.6      |\n",
      "|    reward             | 0.8446291 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 3.44      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 22100      |\n",
      "|    time_elapsed       | 419        |\n",
      "|    total_timesteps    | 110500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 22099      |\n",
      "|    policy_loss        | 54.7       |\n",
      "|    reward             | 0.07691816 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 2.87       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 22200     |\n",
      "|    time_elapsed       | 421       |\n",
      "|    total_timesteps    | 111000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22199     |\n",
      "|    policy_loss        | 90.3      |\n",
      "|    reward             | 0.5075205 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 4.34      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 22300      |\n",
      "|    time_elapsed       | 423        |\n",
      "|    total_timesteps    | 111500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 22299      |\n",
      "|    policy_loss        | 137        |\n",
      "|    reward             | -6.2980423 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 14.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 22400      |\n",
      "|    time_elapsed       | 425        |\n",
      "|    total_timesteps    | 112000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 22399      |\n",
      "|    policy_loss        | -3.36      |\n",
      "|    reward             | 0.18245266 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.152      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 22500     |\n",
      "|    time_elapsed       | 426       |\n",
      "|    total_timesteps    | 112500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22499     |\n",
      "|    policy_loss        | -324      |\n",
      "|    reward             | 3.4521744 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 61.6      |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4440439.38\n",
      "total_reward: 3440439.38\n",
      "total_cost: 7088.80\n",
      "total_trades: 39419\n",
      "Sharpe: 0.839\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 22600     |\n",
      "|    time_elapsed       | 428       |\n",
      "|    total_timesteps    | 113000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22599     |\n",
      "|    policy_loss        | -109      |\n",
      "|    reward             | 0.9091195 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 7.56      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 22700      |\n",
      "|    time_elapsed       | 430        |\n",
      "|    total_timesteps    | 113500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | -3.23e-05  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 22699      |\n",
      "|    policy_loss        | 107        |\n",
      "|    reward             | -3.3824005 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 10.7       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 22800       |\n",
      "|    time_elapsed       | 432         |\n",
      "|    total_timesteps    | 114000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 22799       |\n",
      "|    policy_loss        | -18.2       |\n",
      "|    reward             | 0.043602943 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 0.637       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 263      |\n",
      "|    iterations         | 22900    |\n",
      "|    time_elapsed       | 434      |\n",
      "|    total_timesteps    | 114500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 22899    |\n",
      "|    policy_loss        | -259     |\n",
      "|    reward             | 9.5605   |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 35.6     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 23000      |\n",
      "|    time_elapsed       | 435        |\n",
      "|    total_timesteps    | 115000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 22999      |\n",
      "|    policy_loss        | 3.25       |\n",
      "|    reward             | -0.2718302 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 4.85       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 23100     |\n",
      "|    time_elapsed       | 437       |\n",
      "|    total_timesteps    | 115500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.8     |\n",
      "|    explained_variance | 0.000131  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23099     |\n",
      "|    policy_loss        | 111       |\n",
      "|    reward             | 3.0843453 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 44.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 23200      |\n",
      "|    time_elapsed       | 439        |\n",
      "|    total_timesteps    | 116000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 23199      |\n",
      "|    policy_loss        | -160       |\n",
      "|    reward             | -1.1553977 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 14.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 23300       |\n",
      "|    time_elapsed       | 441         |\n",
      "|    total_timesteps    | 116500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 23299       |\n",
      "|    policy_loss        | -9.21       |\n",
      "|    reward             | -0.10298907 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 0.832       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 23400       |\n",
      "|    time_elapsed       | 443         |\n",
      "|    total_timesteps    | 117000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 23399       |\n",
      "|    policy_loss        | -117        |\n",
      "|    reward             | -0.50810313 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 20.3        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 263      |\n",
      "|    iterations         | 23500    |\n",
      "|    time_elapsed       | 445      |\n",
      "|    total_timesteps    | 117500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.8    |\n",
      "|    explained_variance | 0.0501   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23499    |\n",
      "|    policy_loss        | 123      |\n",
      "|    reward             | 6.852472 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 13.6     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 23600     |\n",
      "|    time_elapsed       | 446       |\n",
      "|    total_timesteps    | 118000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23599     |\n",
      "|    policy_loss        | 571       |\n",
      "|    reward             | 0.5351209 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 171       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 263      |\n",
      "|    iterations         | 23700    |\n",
      "|    time_elapsed       | 448      |\n",
      "|    total_timesteps    | 118500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23699    |\n",
      "|    policy_loss        | -164     |\n",
      "|    reward             | 2.451823 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 20.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 23800      |\n",
      "|    time_elapsed       | 450        |\n",
      "|    total_timesteps    | 119000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 23799      |\n",
      "|    policy_loss        | 119        |\n",
      "|    reward             | 0.19459452 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 10.6       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 263      |\n",
      "|    iterations         | 23900    |\n",
      "|    time_elapsed       | 452      |\n",
      "|    total_timesteps    | 119500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -44.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 23899    |\n",
      "|    policy_loss        | 58       |\n",
      "|    reward             | 0.313318 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 1.84     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 24000     |\n",
      "|    time_elapsed       | 454       |\n",
      "|    total_timesteps    | 120000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23999     |\n",
      "|    policy_loss        | 23        |\n",
      "|    reward             | 1.6451386 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 0.569     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 24100      |\n",
      "|    time_elapsed       | 456        |\n",
      "|    total_timesteps    | 120500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 24099      |\n",
      "|    policy_loss        | -148       |\n",
      "|    reward             | 0.20280023 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 12.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 24200      |\n",
      "|    time_elapsed       | 458        |\n",
      "|    total_timesteps    | 121000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 24199      |\n",
      "|    policy_loss        | -209       |\n",
      "|    reward             | -1.2900051 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 22.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 24300     |\n",
      "|    time_elapsed       | 460       |\n",
      "|    total_timesteps    | 121500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -44.9     |\n",
      "|    explained_variance | 0.016     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24299     |\n",
      "|    policy_loss        | 2.48      |\n",
      "|    reward             | 3.0594401 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 152       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 24400       |\n",
      "|    time_elapsed       | 462         |\n",
      "|    total_timesteps    | 122000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 24399       |\n",
      "|    policy_loss        | -66         |\n",
      "|    reward             | -0.22240765 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 2.48        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 24500      |\n",
      "|    time_elapsed       | 464        |\n",
      "|    total_timesteps    | 122500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 24499      |\n",
      "|    policy_loss        | 13.1       |\n",
      "|    reward             | -0.6376702 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 0.472      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 24600      |\n",
      "|    time_elapsed       | 466        |\n",
      "|    total_timesteps    | 123000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 24599      |\n",
      "|    policy_loss        | 69.3       |\n",
      "|    reward             | -1.2809963 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 7.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 24700      |\n",
      "|    time_elapsed       | 468        |\n",
      "|    total_timesteps    | 123500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 24699      |\n",
      "|    policy_loss        | 141        |\n",
      "|    reward             | 0.26070732 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 10.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 24800     |\n",
      "|    time_elapsed       | 470       |\n",
      "|    total_timesteps    | 124000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24799     |\n",
      "|    policy_loss        | -648      |\n",
      "|    reward             | 10.965261 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 229       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 24900     |\n",
      "|    time_elapsed       | 472       |\n",
      "|    total_timesteps    | 124500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.1     |\n",
      "|    explained_variance | 0.152     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24899     |\n",
      "|    policy_loss        | -106      |\n",
      "|    reward             | 1.1810834 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 6.01      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 25000      |\n",
      "|    time_elapsed       | 473        |\n",
      "|    total_timesteps    | 125000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 24999      |\n",
      "|    policy_loss        | -33.8      |\n",
      "|    reward             | -1.5953673 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 0.743      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 25100       |\n",
      "|    time_elapsed       | 475         |\n",
      "|    total_timesteps    | 125500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 25099       |\n",
      "|    policy_loss        | 21.1        |\n",
      "|    reward             | -0.61491764 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.973       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 25200     |\n",
      "|    time_elapsed       | 477       |\n",
      "|    total_timesteps    | 126000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25199     |\n",
      "|    policy_loss        | 27.1      |\n",
      "|    reward             | 2.9672918 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 2.72      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 25300     |\n",
      "|    time_elapsed       | 479       |\n",
      "|    total_timesteps    | 126500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25299     |\n",
      "|    policy_loss        | 17.3      |\n",
      "|    reward             | 0.9718781 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 0.533     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 25400      |\n",
      "|    time_elapsed       | 481        |\n",
      "|    total_timesteps    | 127000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 25399      |\n",
      "|    policy_loss        | -542       |\n",
      "|    reward             | -1.1074008 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 200        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 25500     |\n",
      "|    time_elapsed       | 483       |\n",
      "|    total_timesteps    | 127500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25499     |\n",
      "|    policy_loss        | 12.3      |\n",
      "|    reward             | 2.0001473 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 0.885     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 25600     |\n",
      "|    time_elapsed       | 485       |\n",
      "|    total_timesteps    | 128000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25599     |\n",
      "|    policy_loss        | -91.9     |\n",
      "|    reward             | 1.7175938 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 5.49      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 263         |\n",
      "|    iterations         | 25700       |\n",
      "|    time_elapsed       | 487         |\n",
      "|    total_timesteps    | 128500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 25699       |\n",
      "|    policy_loss        | 164         |\n",
      "|    reward             | -0.63790673 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 20.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 25800     |\n",
      "|    time_elapsed       | 488       |\n",
      "|    total_timesteps    | 129000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | -0.0438   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25799     |\n",
      "|    policy_loss        | 216       |\n",
      "|    reward             | 1.5464978 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 33.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 25900     |\n",
      "|    time_elapsed       | 490       |\n",
      "|    total_timesteps    | 129500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25899     |\n",
      "|    policy_loss        | 89.7      |\n",
      "|    reward             | 5.9303694 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 7.01      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 26000      |\n",
      "|    time_elapsed       | 492        |\n",
      "|    total_timesteps    | 130000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 25999      |\n",
      "|    policy_loss        | 130        |\n",
      "|    reward             | -3.2689981 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 8.85       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 26100     |\n",
      "|    time_elapsed       | 494       |\n",
      "|    total_timesteps    | 130500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.2     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26099     |\n",
      "|    policy_loss        | 13.4      |\n",
      "|    reward             | 0.1525728 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 0.438     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 26200      |\n",
      "|    time_elapsed       | 496        |\n",
      "|    total_timesteps    | 131000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 26199      |\n",
      "|    policy_loss        | -113       |\n",
      "|    reward             | -1.8089457 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 7.42       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 26300      |\n",
      "|    time_elapsed       | 498        |\n",
      "|    total_timesteps    | 131500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 26299      |\n",
      "|    policy_loss        | -58.4      |\n",
      "|    reward             | -1.7198168 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 4.4        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 26400     |\n",
      "|    time_elapsed       | 500       |\n",
      "|    total_timesteps    | 132000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26399     |\n",
      "|    policy_loss        | 87        |\n",
      "|    reward             | 1.7501594 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 4.46      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 26500     |\n",
      "|    time_elapsed       | 502       |\n",
      "|    total_timesteps    | 132500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26499     |\n",
      "|    policy_loss        | 79.3      |\n",
      "|    reward             | 1.1433784 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 4.66      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 26600      |\n",
      "|    time_elapsed       | 503        |\n",
      "|    total_timesteps    | 133000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 26599      |\n",
      "|    policy_loss        | 708        |\n",
      "|    reward             | -20.501184 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 323        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 26700      |\n",
      "|    time_elapsed       | 505        |\n",
      "|    total_timesteps    | 133500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 26699      |\n",
      "|    policy_loss        | -124       |\n",
      "|    reward             | -1.4877498 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 8.15       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 26800     |\n",
      "|    time_elapsed       | 507       |\n",
      "|    total_timesteps    | 134000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26799     |\n",
      "|    policy_loss        | -75.2     |\n",
      "|    reward             | 0.6541037 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 3.72      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 26900     |\n",
      "|    time_elapsed       | 509       |\n",
      "|    total_timesteps    | 134500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26899     |\n",
      "|    policy_loss        | 110       |\n",
      "|    reward             | 0.3722247 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 8.36      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 27000      |\n",
      "|    time_elapsed       | 511        |\n",
      "|    total_timesteps    | 135000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 26999      |\n",
      "|    policy_loss        | 21.3       |\n",
      "|    reward             | -0.8525813 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 1.77       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 27100     |\n",
      "|    time_elapsed       | 513       |\n",
      "|    total_timesteps    | 135500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27099     |\n",
      "|    policy_loss        | -74.8     |\n",
      "|    reward             | 16.080078 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 6.61      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 27200      |\n",
      "|    time_elapsed       | 515        |\n",
      "|    total_timesteps    | 136000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 27199      |\n",
      "|    policy_loss        | 93.2       |\n",
      "|    reward             | -0.6896545 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 4.2        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 27300     |\n",
      "|    time_elapsed       | 517       |\n",
      "|    total_timesteps    | 136500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27299     |\n",
      "|    policy_loss        | -19       |\n",
      "|    reward             | 1.0175192 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 1.67      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 27400     |\n",
      "|    time_elapsed       | 519       |\n",
      "|    total_timesteps    | 137000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27399     |\n",
      "|    policy_loss        | 76.4      |\n",
      "|    reward             | 1.9931198 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 2.76      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 27500     |\n",
      "|    time_elapsed       | 521       |\n",
      "|    total_timesteps    | 137500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27499     |\n",
      "|    policy_loss        | 123       |\n",
      "|    reward             | -7.145242 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 12.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 27600     |\n",
      "|    time_elapsed       | 522       |\n",
      "|    total_timesteps    | 138000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27599     |\n",
      "|    policy_loss        | 10.8      |\n",
      "|    reward             | 0.7769831 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 0.957     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 27700     |\n",
      "|    time_elapsed       | 524       |\n",
      "|    total_timesteps    | 138500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27699     |\n",
      "|    policy_loss        | -18.3     |\n",
      "|    reward             | -4.119868 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 2.26      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 263       |\n",
      "|    iterations         | 27800     |\n",
      "|    time_elapsed       | 526       |\n",
      "|    total_timesteps    | 139000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 0.239     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27799     |\n",
      "|    policy_loss        | 32        |\n",
      "|    reward             | 1.2892113 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 1.04      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 27900     |\n",
      "|    time_elapsed       | 528       |\n",
      "|    total_timesteps    | 139500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27899     |\n",
      "|    policy_loss        | 126       |\n",
      "|    reward             | 0.5506836 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 10        |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 28000       |\n",
      "|    time_elapsed       | 530         |\n",
      "|    total_timesteps    | 140000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 27999       |\n",
      "|    policy_loss        | -47.7       |\n",
      "|    reward             | 0.029980985 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 2.2         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 28100     |\n",
      "|    time_elapsed       | 532       |\n",
      "|    total_timesteps    | 140500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28099     |\n",
      "|    policy_loss        | -39.3     |\n",
      "|    reward             | 0.5160061 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 4.4       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 28200     |\n",
      "|    time_elapsed       | 533       |\n",
      "|    total_timesteps    | 141000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28199     |\n",
      "|    policy_loss        | -60.8     |\n",
      "|    reward             | 1.3896471 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 9.18      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 28300      |\n",
      "|    time_elapsed       | 535        |\n",
      "|    total_timesteps    | 141500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 28299      |\n",
      "|    policy_loss        | 191        |\n",
      "|    reward             | -2.5396614 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 17.1       |\n",
      "--------------------------------------\n",
      "day: 2892, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4081760.28\n",
      "total_reward: 3081760.28\n",
      "total_cost: 5102.01\n",
      "total_trades: 44490\n",
      "Sharpe: 0.856\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 28400      |\n",
      "|    time_elapsed       | 537        |\n",
      "|    total_timesteps    | 142000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 28399      |\n",
      "|    policy_loss        | 84.1       |\n",
      "|    reward             | 0.59719455 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 4.24       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 28500      |\n",
      "|    time_elapsed       | 539        |\n",
      "|    total_timesteps    | 142500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 28499      |\n",
      "|    policy_loss        | 89.8       |\n",
      "|    reward             | -1.8256928 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 6.31       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 28600      |\n",
      "|    time_elapsed       | 541        |\n",
      "|    total_timesteps    | 143000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.8      |\n",
      "|    explained_variance | 0.0022     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 28599      |\n",
      "|    policy_loss        | -162       |\n",
      "|    reward             | -2.1646047 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 15.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 28700     |\n",
      "|    time_elapsed       | 543       |\n",
      "|    total_timesteps    | 143500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28699     |\n",
      "|    policy_loss        | -3.12     |\n",
      "|    reward             | 5.4491525 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 0.741     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 28800     |\n",
      "|    time_elapsed       | 544       |\n",
      "|    total_timesteps    | 144000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28799     |\n",
      "|    policy_loss        | 281       |\n",
      "|    reward             | 3.9724047 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 109       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 28900     |\n",
      "|    time_elapsed       | 546       |\n",
      "|    total_timesteps    | 144500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28899     |\n",
      "|    policy_loss        | 163       |\n",
      "|    reward             | 1.1223785 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 16.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 29000       |\n",
      "|    time_elapsed       | 548         |\n",
      "|    total_timesteps    | 145000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 28999       |\n",
      "|    policy_loss        | -74.5       |\n",
      "|    reward             | -0.39191726 |\n",
      "|    std                | 1.18        |\n",
      "|    value_loss         | 4.37        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 29100      |\n",
      "|    time_elapsed       | 550        |\n",
      "|    total_timesteps    | 145500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 29099      |\n",
      "|    policy_loss        | -50.9      |\n",
      "|    reward             | 0.92842877 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 2.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 29200      |\n",
      "|    time_elapsed       | 552        |\n",
      "|    total_timesteps    | 146000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 29199      |\n",
      "|    policy_loss        | 26.5       |\n",
      "|    reward             | -3.1134167 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.91       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 29300     |\n",
      "|    time_elapsed       | 554       |\n",
      "|    total_timesteps    | 146500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29299     |\n",
      "|    policy_loss        | -68.1     |\n",
      "|    reward             | 1.9376159 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 4.92      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 29400     |\n",
      "|    time_elapsed       | 556       |\n",
      "|    total_timesteps    | 147000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29399     |\n",
      "|    policy_loss        | -175      |\n",
      "|    reward             | 6.5775604 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 50.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 29500      |\n",
      "|    time_elapsed       | 557        |\n",
      "|    total_timesteps    | 147500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 29499      |\n",
      "|    policy_loss        | -133       |\n",
      "|    reward             | -7.4048066 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 35.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 29600     |\n",
      "|    time_elapsed       | 559       |\n",
      "|    total_timesteps    | 148000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29599     |\n",
      "|    policy_loss        | -64.8     |\n",
      "|    reward             | 0.2594582 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 2.23      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 264          |\n",
      "|    iterations         | 29700        |\n",
      "|    time_elapsed       | 561          |\n",
      "|    total_timesteps    | 148500       |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -45.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 29699        |\n",
      "|    policy_loss        | -99          |\n",
      "|    reward             | -0.059541654 |\n",
      "|    std                | 1.18         |\n",
      "|    value_loss         | 5.49         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 29800      |\n",
      "|    time_elapsed       | 563        |\n",
      "|    total_timesteps    | 149000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -45.9      |\n",
      "|    explained_variance | 0.0109     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 29799      |\n",
      "|    policy_loss        | -85.9      |\n",
      "|    reward             | -0.6916573 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 7.75       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 29900     |\n",
      "|    time_elapsed       | 565       |\n",
      "|    total_timesteps    | 149500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29899     |\n",
      "|    policy_loss        | -224      |\n",
      "|    reward             | -2.889723 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 28.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 30000      |\n",
      "|    time_elapsed       | 567        |\n",
      "|    total_timesteps    | 150000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 29999      |\n",
      "|    policy_loss        | -172       |\n",
      "|    reward             | -1.9799539 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 27.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 30100       |\n",
      "|    time_elapsed       | 568         |\n",
      "|    total_timesteps    | 150500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 30099       |\n",
      "|    policy_loss        | -68.7       |\n",
      "|    reward             | -0.27279153 |\n",
      "|    std                | 1.19        |\n",
      "|    value_loss         | 2.73        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 30200       |\n",
      "|    time_elapsed       | 570         |\n",
      "|    total_timesteps    | 151000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 30199       |\n",
      "|    policy_loss        | 22.1        |\n",
      "|    reward             | 0.064138055 |\n",
      "|    std                | 1.19        |\n",
      "|    value_loss         | 0.601       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 30300      |\n",
      "|    time_elapsed       | 572        |\n",
      "|    total_timesteps    | 151500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 30299      |\n",
      "|    policy_loss        | 15.5       |\n",
      "|    reward             | -0.9046903 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.13       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 30400       |\n",
      "|    time_elapsed       | 574         |\n",
      "|    total_timesteps    | 152000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 30399       |\n",
      "|    policy_loss        | 196         |\n",
      "|    reward             | -0.33059025 |\n",
      "|    std                | 1.19        |\n",
      "|    value_loss         | 23.7        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 30500      |\n",
      "|    time_elapsed       | 576        |\n",
      "|    total_timesteps    | 152500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 30499      |\n",
      "|    policy_loss        | 9.18       |\n",
      "|    reward             | -1.0560932 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 30600     |\n",
      "|    time_elapsed       | 578       |\n",
      "|    total_timesteps    | 153000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 30599     |\n",
      "|    policy_loss        | -238      |\n",
      "|    reward             | 2.5535955 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 31.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 30700      |\n",
      "|    time_elapsed       | 580        |\n",
      "|    total_timesteps    | 153500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 30699      |\n",
      "|    policy_loss        | -50.4      |\n",
      "|    reward             | 0.97478396 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 1.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 30800     |\n",
      "|    time_elapsed       | 581       |\n",
      "|    total_timesteps    | 154000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 30799     |\n",
      "|    policy_loss        | 111       |\n",
      "|    reward             | 0.9887007 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 9.06      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 30900       |\n",
      "|    time_elapsed       | 583         |\n",
      "|    total_timesteps    | 154500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.3       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 30899       |\n",
      "|    policy_loss        | -161        |\n",
      "|    reward             | -0.05827682 |\n",
      "|    std                | 1.2         |\n",
      "|    value_loss         | 16.6        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 31000     |\n",
      "|    time_elapsed       | 585       |\n",
      "|    total_timesteps    | 155000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 30999     |\n",
      "|    policy_loss        | 15.8      |\n",
      "|    reward             | -8.450091 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 2.08      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 31100     |\n",
      "|    time_elapsed       | 587       |\n",
      "|    total_timesteps    | 155500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31099     |\n",
      "|    policy_loss        | -151      |\n",
      "|    reward             | -4.406966 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 11.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 31200     |\n",
      "|    time_elapsed       | 590       |\n",
      "|    total_timesteps    | 156000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31199     |\n",
      "|    policy_loss        | -329      |\n",
      "|    reward             | -9.670143 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 93.9      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 264          |\n",
      "|    iterations         | 31300        |\n",
      "|    time_elapsed       | 591          |\n",
      "|    total_timesteps    | 156500       |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -46.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 31299        |\n",
      "|    policy_loss        | 25.9         |\n",
      "|    reward             | -0.034307215 |\n",
      "|    std                | 1.2          |\n",
      "|    value_loss         | 0.754        |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 264      |\n",
      "|    iterations         | 31400    |\n",
      "|    time_elapsed       | 593      |\n",
      "|    total_timesteps    | 157000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -46.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 31399    |\n",
      "|    policy_loss        | -18.1    |\n",
      "|    reward             | 1.029369 |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 0.254    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 31500      |\n",
      "|    time_elapsed       | 595        |\n",
      "|    total_timesteps    | 157500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 31499      |\n",
      "|    policy_loss        | -405       |\n",
      "|    reward             | -1.0408089 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 74.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 31600      |\n",
      "|    time_elapsed       | 597        |\n",
      "|    total_timesteps    | 158000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 31599      |\n",
      "|    policy_loss        | -325       |\n",
      "|    reward             | -1.2217563 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 62.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 31700     |\n",
      "|    time_elapsed       | 599       |\n",
      "|    total_timesteps    | 158500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31699     |\n",
      "|    policy_loss        | 220       |\n",
      "|    reward             | 1.0773199 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 24.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 31800     |\n",
      "|    time_elapsed       | 601       |\n",
      "|    total_timesteps    | 159000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31799     |\n",
      "|    policy_loss        | -37.2     |\n",
      "|    reward             | 3.001138  |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 6.76      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 31900       |\n",
      "|    time_elapsed       | 602         |\n",
      "|    total_timesteps    | 159500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 31899       |\n",
      "|    policy_loss        | 155         |\n",
      "|    reward             | -0.04774947 |\n",
      "|    std                | 1.2         |\n",
      "|    value_loss         | 12.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 32000      |\n",
      "|    time_elapsed       | 604        |\n",
      "|    total_timesteps    | 160000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.2      |\n",
      "|    explained_variance | -0.0067    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 31999      |\n",
      "|    policy_loss        | 80.7       |\n",
      "|    reward             | 0.20657231 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 3.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 32100     |\n",
      "|    time_elapsed       | 606       |\n",
      "|    total_timesteps    | 160500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32099     |\n",
      "|    policy_loss        | 27.3      |\n",
      "|    reward             | 0.1538468 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 0.712     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 32200     |\n",
      "|    time_elapsed       | 608       |\n",
      "|    total_timesteps    | 161000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32199     |\n",
      "|    policy_loss        | 65.9      |\n",
      "|    reward             | 5.0494285 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 3.38      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 32300       |\n",
      "|    time_elapsed       | 610         |\n",
      "|    total_timesteps    | 161500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.3       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 32299       |\n",
      "|    policy_loss        | -87.9       |\n",
      "|    reward             | -0.58821154 |\n",
      "|    std                | 1.2         |\n",
      "|    value_loss         | 4.14        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 32400      |\n",
      "|    time_elapsed       | 612        |\n",
      "|    total_timesteps    | 162000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 32399      |\n",
      "|    policy_loss        | -873       |\n",
      "|    reward             | -1.6062824 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 482        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 32500       |\n",
      "|    time_elapsed       | 614         |\n",
      "|    total_timesteps    | 162500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 32499       |\n",
      "|    policy_loss        | -53.2       |\n",
      "|    reward             | -0.51571083 |\n",
      "|    std                | 1.2         |\n",
      "|    value_loss         | 1.7         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 32600      |\n",
      "|    time_elapsed       | 615        |\n",
      "|    total_timesteps    | 163000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 32599      |\n",
      "|    policy_loss        | 23.4       |\n",
      "|    reward             | 0.45021534 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 1.59       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 32700      |\n",
      "|    time_elapsed       | 617        |\n",
      "|    total_timesteps    | 163500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 32699      |\n",
      "|    policy_loss        | 254        |\n",
      "|    reward             | 0.12249849 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 28.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 32800      |\n",
      "|    time_elapsed       | 619        |\n",
      "|    total_timesteps    | 164000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 32799      |\n",
      "|    policy_loss        | -15.4      |\n",
      "|    reward             | -3.1323388 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 1.79       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 32900     |\n",
      "|    time_elapsed       | 621       |\n",
      "|    total_timesteps    | 164500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32899     |\n",
      "|    policy_loss        | -108      |\n",
      "|    reward             | 3.8739216 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 16.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 33000      |\n",
      "|    time_elapsed       | 623        |\n",
      "|    total_timesteps    | 165000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.6      |\n",
      "|    explained_variance | 0.102      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 32999      |\n",
      "|    policy_loss        | 20.6       |\n",
      "|    reward             | -2.2081745 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 1.76       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 33100      |\n",
      "|    time_elapsed       | 625        |\n",
      "|    total_timesteps    | 165500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 33099      |\n",
      "|    policy_loss        | 36.4       |\n",
      "|    reward             | 0.90254754 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 1.05       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 33200      |\n",
      "|    time_elapsed       | 626        |\n",
      "|    total_timesteps    | 166000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 33199      |\n",
      "|    policy_loss        | 84.6       |\n",
      "|    reward             | -3.0237224 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 4.45       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 33300     |\n",
      "|    time_elapsed       | 628       |\n",
      "|    total_timesteps    | 166500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33299     |\n",
      "|    policy_loss        | -20.9     |\n",
      "|    reward             | 1.1685902 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 3.68      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 33400      |\n",
      "|    time_elapsed       | 630        |\n",
      "|    total_timesteps    | 167000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 33399      |\n",
      "|    policy_loss        | 53.4       |\n",
      "|    reward             | 0.29660258 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 3.59       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 33500     |\n",
      "|    time_elapsed       | 632       |\n",
      "|    total_timesteps    | 167500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.7     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33499     |\n",
      "|    policy_loss        | -799      |\n",
      "|    reward             | -3.336607 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 284       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 33600      |\n",
      "|    time_elapsed       | 634        |\n",
      "|    total_timesteps    | 168000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 33599      |\n",
      "|    policy_loss        | -15.7      |\n",
      "|    reward             | -0.4161531 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 0.626      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 33700     |\n",
      "|    time_elapsed       | 636       |\n",
      "|    total_timesteps    | 168500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33699     |\n",
      "|    policy_loss        | 94.6      |\n",
      "|    reward             | 0.5884834 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 8.52      |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 264            |\n",
      "|    iterations         | 33800          |\n",
      "|    time_elapsed       | 637            |\n",
      "|    total_timesteps    | 169000         |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -46.8          |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 33799          |\n",
      "|    policy_loss        | 172            |\n",
      "|    reward             | -3.5263063e-05 |\n",
      "|    std                | 1.22           |\n",
      "|    value_loss         | 16.2           |\n",
      "------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 33900      |\n",
      "|    time_elapsed       | 639        |\n",
      "|    total_timesteps    | 169500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 33899      |\n",
      "|    policy_loss        | 136        |\n",
      "|    reward             | -1.1632401 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 20         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 34000       |\n",
      "|    time_elapsed       | 641         |\n",
      "|    total_timesteps    | 170000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 33999       |\n",
      "|    policy_loss        | -139        |\n",
      "|    reward             | -0.98158264 |\n",
      "|    std                | 1.22        |\n",
      "|    value_loss         | 10.9        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 265         |\n",
      "|    iterations         | 34100       |\n",
      "|    time_elapsed       | 643         |\n",
      "|    total_timesteps    | 170500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 34099       |\n",
      "|    policy_loss        | -4.19       |\n",
      "|    reward             | -0.20340945 |\n",
      "|    std                | 1.22        |\n",
      "|    value_loss         | 1.06        |\n",
      "---------------------------------------\n",
      "day: 2892, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4342288.50\n",
      "total_reward: 3342288.50\n",
      "total_cost: 4113.28\n",
      "total_trades: 39219\n",
      "Sharpe: 0.887\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 34200     |\n",
      "|    time_elapsed       | 645       |\n",
      "|    total_timesteps    | 171000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.8     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34199     |\n",
      "|    policy_loss        | -26.2     |\n",
      "|    reward             | 0.8169887 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 0.676     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 34300     |\n",
      "|    time_elapsed       | 647       |\n",
      "|    total_timesteps    | 171500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34299     |\n",
      "|    policy_loss        | 13.3      |\n",
      "|    reward             | 1.8042846 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 0.716     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 34400      |\n",
      "|    time_elapsed       | 648        |\n",
      "|    total_timesteps    | 172000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 34399      |\n",
      "|    policy_loss        | 8.45       |\n",
      "|    reward             | -0.5624058 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 3.01       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 34500     |\n",
      "|    time_elapsed       | 650       |\n",
      "|    total_timesteps    | 172500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34499     |\n",
      "|    policy_loss        | -44.2     |\n",
      "|    reward             | 1.3799416 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 1.16      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 34600      |\n",
      "|    time_elapsed       | 652        |\n",
      "|    total_timesteps    | 173000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 34599      |\n",
      "|    policy_loss        | -102       |\n",
      "|    reward             | -1.3357259 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 26.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 34700      |\n",
      "|    time_elapsed       | 654        |\n",
      "|    total_timesteps    | 173500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.8      |\n",
      "|    explained_variance | 9.63e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 34699      |\n",
      "|    policy_loss        | 204        |\n",
      "|    reward             | -17.828629 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 153        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 34800     |\n",
      "|    time_elapsed       | 656       |\n",
      "|    total_timesteps    | 174000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -46.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34799     |\n",
      "|    policy_loss        | -70.3     |\n",
      "|    reward             | 1.2643182 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 2.58      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 34900      |\n",
      "|    time_elapsed       | 657        |\n",
      "|    total_timesteps    | 174500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 34899      |\n",
      "|    policy_loss        | -35        |\n",
      "|    reward             | 0.17672345 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 0.96       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 35000      |\n",
      "|    time_elapsed       | 659        |\n",
      "|    total_timesteps    | 175000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 34999      |\n",
      "|    policy_loss        | 121        |\n",
      "|    reward             | 0.33247867 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 6.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 35100     |\n",
      "|    time_elapsed       | 661       |\n",
      "|    total_timesteps    | 175500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35099     |\n",
      "|    policy_loss        | 143       |\n",
      "|    reward             | 1.2638303 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 9.76      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 35200     |\n",
      "|    time_elapsed       | 663       |\n",
      "|    total_timesteps    | 176000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35199     |\n",
      "|    policy_loss        | 130       |\n",
      "|    reward             | 2.4661117 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 10.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 35300      |\n",
      "|    time_elapsed       | 665        |\n",
      "|    total_timesteps    | 176500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 35299      |\n",
      "|    policy_loss        | -23.6      |\n",
      "|    reward             | 0.42065048 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 0.46       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 35400     |\n",
      "|    time_elapsed       | 667       |\n",
      "|    total_timesteps    | 177000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35399     |\n",
      "|    policy_loss        | -119      |\n",
      "|    reward             | 0.4835221 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 6.09      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 35500     |\n",
      "|    time_elapsed       | 669       |\n",
      "|    total_timesteps    | 177500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35499     |\n",
      "|    policy_loss        | 51.1      |\n",
      "|    reward             | 1.3371685 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 1.9       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 35600      |\n",
      "|    time_elapsed       | 671        |\n",
      "|    total_timesteps    | 178000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 35599      |\n",
      "|    policy_loss        | -28.9      |\n",
      "|    reward             | -1.6657743 |\n",
      "|    std                | 1.24       |\n",
      "|    value_loss         | 4.59       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 35700      |\n",
      "|    time_elapsed       | 672        |\n",
      "|    total_timesteps    | 178500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 35699      |\n",
      "|    policy_loss        | -134       |\n",
      "|    reward             | 0.23828392 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 7.18       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 35800     |\n",
      "|    time_elapsed       | 674       |\n",
      "|    total_timesteps    | 179000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35799     |\n",
      "|    policy_loss        | 255       |\n",
      "|    reward             | 1.7572267 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 67.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 35900     |\n",
      "|    time_elapsed       | 676       |\n",
      "|    total_timesteps    | 179500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | 0.00174   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35899     |\n",
      "|    policy_loss        | -60.5     |\n",
      "|    reward             | 0.8469045 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 2.4       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 36000      |\n",
      "|    time_elapsed       | 678        |\n",
      "|    total_timesteps    | 180000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.1      |\n",
      "|    explained_variance | 0.000677   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 35999      |\n",
      "|    policy_loss        | 181        |\n",
      "|    reward             | -1.2767793 |\n",
      "|    std                | 1.24       |\n",
      "|    value_loss         | 15.9       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 265      |\n",
      "|    iterations         | 36100    |\n",
      "|    time_elapsed       | 680      |\n",
      "|    total_timesteps    | 180500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -47.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 36099    |\n",
      "|    policy_loss        | 62.4     |\n",
      "|    reward             | 2.77071  |\n",
      "|    std                | 1.24     |\n",
      "|    value_loss         | 4.69     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 36200     |\n",
      "|    time_elapsed       | 682       |\n",
      "|    total_timesteps    | 181000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36199     |\n",
      "|    policy_loss        | -87       |\n",
      "|    reward             | 3.0823271 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 5.14      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 36300     |\n",
      "|    time_elapsed       | 684       |\n",
      "|    total_timesteps    | 181500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.1     |\n",
      "|    explained_variance | 0.000837  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36299     |\n",
      "|    policy_loss        | -83.9     |\n",
      "|    reward             | 1.0334897 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 8.6       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 36400      |\n",
      "|    time_elapsed       | 686        |\n",
      "|    total_timesteps    | 182000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 36399      |\n",
      "|    policy_loss        | 174        |\n",
      "|    reward             | 0.75853914 |\n",
      "|    std                | 1.24       |\n",
      "|    value_loss         | 19.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 36500      |\n",
      "|    time_elapsed       | 688        |\n",
      "|    total_timesteps    | 182500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 36499      |\n",
      "|    policy_loss        | -2.39      |\n",
      "|    reward             | -0.8966923 |\n",
      "|    std                | 1.24       |\n",
      "|    value_loss         | 0.424      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 36600     |\n",
      "|    time_elapsed       | 690       |\n",
      "|    total_timesteps    | 183000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.2     |\n",
      "|    explained_variance | -0.00184  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36599     |\n",
      "|    policy_loss        | 70.5      |\n",
      "|    reward             | 2.0482063 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 10.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 36700     |\n",
      "|    time_elapsed       | 691       |\n",
      "|    total_timesteps    | 183500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36699     |\n",
      "|    policy_loss        | 67.9      |\n",
      "|    reward             | 2.9008832 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 3.64      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 36800      |\n",
      "|    time_elapsed       | 693        |\n",
      "|    total_timesteps    | 184000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 36799      |\n",
      "|    policy_loss        | 122        |\n",
      "|    reward             | -1.3302778 |\n",
      "|    std                | 1.24       |\n",
      "|    value_loss         | 12.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 36900     |\n",
      "|    time_elapsed       | 695       |\n",
      "|    total_timesteps    | 184500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.3     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36899     |\n",
      "|    policy_loss        | 368       |\n",
      "|    reward             | 2.4449663 |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 78        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 37000      |\n",
      "|    time_elapsed       | 697        |\n",
      "|    total_timesteps    | 185000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 36999      |\n",
      "|    policy_loss        | 99.9       |\n",
      "|    reward             | -0.9475915 |\n",
      "|    std                | 1.25       |\n",
      "|    value_loss         | 8.86       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 37100      |\n",
      "|    time_elapsed       | 699        |\n",
      "|    total_timesteps    | 185500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.4      |\n",
      "|    explained_variance | -0.000503  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 37099      |\n",
      "|    policy_loss        | -79.4      |\n",
      "|    reward             | 0.87812287 |\n",
      "|    std                | 1.25       |\n",
      "|    value_loss         | 4.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 37200      |\n",
      "|    time_elapsed       | 701        |\n",
      "|    total_timesteps    | 186000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.5      |\n",
      "|    explained_variance | 0.00355    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 37199      |\n",
      "|    policy_loss        | -120       |\n",
      "|    reward             | -0.6992661 |\n",
      "|    std                | 1.25       |\n",
      "|    value_loss         | 7.6        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 265         |\n",
      "|    iterations         | 37300       |\n",
      "|    time_elapsed       | 703         |\n",
      "|    total_timesteps    | 186500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -47.5       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 37299       |\n",
      "|    policy_loss        | -194        |\n",
      "|    reward             | -0.23292488 |\n",
      "|    std                | 1.25        |\n",
      "|    value_loss         | 18          |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 37400     |\n",
      "|    time_elapsed       | 704       |\n",
      "|    total_timesteps    | 187000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 37399     |\n",
      "|    policy_loss        | -243      |\n",
      "|    reward             | 1.9281807 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 27.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 37500      |\n",
      "|    time_elapsed       | 706        |\n",
      "|    total_timesteps    | 187500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 37499      |\n",
      "|    policy_loss        | -307       |\n",
      "|    reward             | -2.2676976 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 58         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 37600      |\n",
      "|    time_elapsed       | 708        |\n",
      "|    total_timesteps    | 188000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 37599      |\n",
      "|    policy_loss        | -501       |\n",
      "|    reward             | -2.2108185 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 145        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 37700     |\n",
      "|    time_elapsed       | 710       |\n",
      "|    total_timesteps    | 188500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 37699     |\n",
      "|    policy_loss        | -38.4     |\n",
      "|    reward             | -0.265311 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 0.949     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 37800     |\n",
      "|    time_elapsed       | 712       |\n",
      "|    total_timesteps    | 189000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 37799     |\n",
      "|    policy_loss        | -74.1     |\n",
      "|    reward             | 1.1973006 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 3.86      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 265         |\n",
      "|    iterations         | 37900       |\n",
      "|    time_elapsed       | 714         |\n",
      "|    total_timesteps    | 189500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -47.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 37899       |\n",
      "|    policy_loss        | -124        |\n",
      "|    reward             | -0.45860392 |\n",
      "|    std                | 1.26        |\n",
      "|    value_loss         | 8.72        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 38000      |\n",
      "|    time_elapsed       | 715        |\n",
      "|    total_timesteps    | 190000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 37999      |\n",
      "|    policy_loss        | -35.1      |\n",
      "|    reward             | -1.2269639 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 5.16       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 38100      |\n",
      "|    time_elapsed       | 717        |\n",
      "|    total_timesteps    | 190500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 38099      |\n",
      "|    policy_loss        | -275       |\n",
      "|    reward             | -2.5674405 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 44.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 38200     |\n",
      "|    time_elapsed       | 719       |\n",
      "|    total_timesteps    | 191000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.7     |\n",
      "|    explained_variance | -7.49     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38199     |\n",
      "|    policy_loss        | 1.14e+03  |\n",
      "|    reward             | 2.1498542 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 562       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 265         |\n",
      "|    iterations         | 38300       |\n",
      "|    time_elapsed       | 721         |\n",
      "|    total_timesteps    | 191500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -47.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 38299       |\n",
      "|    policy_loss        | 18.2        |\n",
      "|    reward             | -0.59945554 |\n",
      "|    std                | 1.26        |\n",
      "|    value_loss         | 3.15        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 38400      |\n",
      "|    time_elapsed       | 723        |\n",
      "|    total_timesteps    | 192000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 38399      |\n",
      "|    policy_loss        | -120       |\n",
      "|    reward             | 0.20156416 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 5.76       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 38500      |\n",
      "|    time_elapsed       | 725        |\n",
      "|    total_timesteps    | 192500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 38499      |\n",
      "|    policy_loss        | 12.9       |\n",
      "|    reward             | -1.6714365 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 12.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 38600      |\n",
      "|    time_elapsed       | 727        |\n",
      "|    total_timesteps    | 193000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 38599      |\n",
      "|    policy_loss        | -79.1      |\n",
      "|    reward             | 0.85632116 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 4.01       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 38700     |\n",
      "|    time_elapsed       | 729       |\n",
      "|    total_timesteps    | 193500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38699     |\n",
      "|    policy_loss        | -267      |\n",
      "|    reward             | 6.5152655 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 33.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 38800     |\n",
      "|    time_elapsed       | 730       |\n",
      "|    total_timesteps    | 194000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38799     |\n",
      "|    policy_loss        | -20.9     |\n",
      "|    reward             | 0.5704439 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 0.329     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 38900     |\n",
      "|    time_elapsed       | 732       |\n",
      "|    total_timesteps    | 194500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.9     |\n",
      "|    explained_variance | 0.0229    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38899     |\n",
      "|    policy_loss        | -138      |\n",
      "|    reward             | 2.2384732 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 13.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 39000     |\n",
      "|    time_elapsed       | 734       |\n",
      "|    total_timesteps    | 195000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -47.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38999     |\n",
      "|    policy_loss        | -226      |\n",
      "|    reward             | -0.572217 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 36.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 39100      |\n",
      "|    time_elapsed       | 736        |\n",
      "|    total_timesteps    | 195500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 39099      |\n",
      "|    policy_loss        | -14.9      |\n",
      "|    reward             | -4.6179075 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 0.852      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 39200      |\n",
      "|    time_elapsed       | 738        |\n",
      "|    total_timesteps    | 196000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -47.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 39199      |\n",
      "|    policy_loss        | 31.4       |\n",
      "|    reward             | -2.2237275 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 39300      |\n",
      "|    time_elapsed       | 740        |\n",
      "|    total_timesteps    | 196500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 39299      |\n",
      "|    policy_loss        | -1.04e+03  |\n",
      "|    reward             | -6.1876307 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 474        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 39400      |\n",
      "|    time_elapsed       | 741        |\n",
      "|    total_timesteps    | 197000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 39399      |\n",
      "|    policy_loss        | -20        |\n",
      "|    reward             | -1.0486529 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 0.969      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 39500     |\n",
      "|    time_elapsed       | 745       |\n",
      "|    total_timesteps    | 197500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39499     |\n",
      "|    policy_loss        | -0.0873   |\n",
      "|    reward             | 1.4586705 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 0.107     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 39600      |\n",
      "|    time_elapsed       | 746        |\n",
      "|    total_timesteps    | 198000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 39599      |\n",
      "|    policy_loss        | -144       |\n",
      "|    reward             | -1.9662291 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 11.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 39700      |\n",
      "|    time_elapsed       | 748        |\n",
      "|    total_timesteps    | 198500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 39699      |\n",
      "|    policy_loss        | -287       |\n",
      "|    reward             | -1.0708872 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 44.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 39800     |\n",
      "|    time_elapsed       | 750       |\n",
      "|    total_timesteps    | 199000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39799     |\n",
      "|    policy_loss        | 4.89      |\n",
      "|    reward             | 1.0996976 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 6.19      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 39900     |\n",
      "|    time_elapsed       | 752       |\n",
      "|    total_timesteps    | 199500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39899     |\n",
      "|    policy_loss        | -274      |\n",
      "|    reward             | 1.4527533 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 32.7      |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5512716.07\n",
      "total_reward: 4512716.07\n",
      "total_cost: 4251.53\n",
      "total_trades: 41651\n",
      "Sharpe: 1.021\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 40000     |\n",
      "|    time_elapsed       | 754       |\n",
      "|    total_timesteps    | 200000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39999     |\n",
      "|    policy_loss        | -58.9     |\n",
      "|    reward             | 1.4508665 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 4.73      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 40100     |\n",
      "|    time_elapsed       | 756       |\n",
      "|    total_timesteps    | 200500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40099     |\n",
      "|    policy_loss        | 12.4      |\n",
      "|    reward             | 0.6380817 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 1.31      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 40200     |\n",
      "|    time_elapsed       | 758       |\n",
      "|    total_timesteps    | 201000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40199     |\n",
      "|    policy_loss        | -11.5     |\n",
      "|    reward             | 1.0077356 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 0.231     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 40300      |\n",
      "|    time_elapsed       | 759        |\n",
      "|    total_timesteps    | 201500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 40299      |\n",
      "|    policy_loss        | 48.7       |\n",
      "|    reward             | -3.2711878 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 1.75       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 40400      |\n",
      "|    time_elapsed       | 761        |\n",
      "|    total_timesteps    | 202000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 40399      |\n",
      "|    policy_loss        | -45.8      |\n",
      "|    reward             | -0.7149875 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 1.06       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 40500      |\n",
      "|    time_elapsed       | 763        |\n",
      "|    total_timesteps    | 202500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 40499      |\n",
      "|    policy_loss        | -103       |\n",
      "|    reward             | -0.9004537 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 15.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 40600     |\n",
      "|    time_elapsed       | 765       |\n",
      "|    total_timesteps    | 203000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40599     |\n",
      "|    policy_loss        | 48.4      |\n",
      "|    reward             | 0.7775314 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 3.59      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 40700     |\n",
      "|    time_elapsed       | 767       |\n",
      "|    total_timesteps    | 203500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40699     |\n",
      "|    policy_loss        | 112       |\n",
      "|    reward             | 1.2285258 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 5.64      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 265         |\n",
      "|    iterations         | 40800       |\n",
      "|    time_elapsed       | 769         |\n",
      "|    total_timesteps    | 204000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -48.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 40799       |\n",
      "|    policy_loss        | -14.7       |\n",
      "|    reward             | -0.41140905 |\n",
      "|    std                | 1.28        |\n",
      "|    value_loss         | 0.839       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 40900      |\n",
      "|    time_elapsed       | 771        |\n",
      "|    total_timesteps    | 204500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 40899      |\n",
      "|    policy_loss        | -181       |\n",
      "|    reward             | -1.4004401 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 17         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 41000      |\n",
      "|    time_elapsed       | 772        |\n",
      "|    total_timesteps    | 205000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 40999      |\n",
      "|    policy_loss        | -626       |\n",
      "|    reward             | 0.49841163 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 174        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 41100     |\n",
      "|    time_elapsed       | 774       |\n",
      "|    total_timesteps    | 205500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41099     |\n",
      "|    policy_loss        | -83.9     |\n",
      "|    reward             | 0.3334144 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 4.62      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 41200      |\n",
      "|    time_elapsed       | 776        |\n",
      "|    total_timesteps    | 206000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 41199      |\n",
      "|    policy_loss        | 11.4       |\n",
      "|    reward             | -0.9052131 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 0.601      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 41300     |\n",
      "|    time_elapsed       | 778       |\n",
      "|    total_timesteps    | 206500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41299     |\n",
      "|    policy_loss        | 60.2      |\n",
      "|    reward             | 1.2220719 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 3.42      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 41400     |\n",
      "|    time_elapsed       | 780       |\n",
      "|    total_timesteps    | 207000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41399     |\n",
      "|    policy_loss        | -35.2     |\n",
      "|    reward             | 3.0755563 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 6.19      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 41500     |\n",
      "|    time_elapsed       | 782       |\n",
      "|    total_timesteps    | 207500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41499     |\n",
      "|    policy_loss        | 148       |\n",
      "|    reward             | 0.8922527 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 12.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 41600     |\n",
      "|    time_elapsed       | 784       |\n",
      "|    total_timesteps    | 208000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41599     |\n",
      "|    policy_loss        | 32.5      |\n",
      "|    reward             | 3.6898952 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 2.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 41700      |\n",
      "|    time_elapsed       | 786        |\n",
      "|    total_timesteps    | 208500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 41699      |\n",
      "|    policy_loss        | -5.6       |\n",
      "|    reward             | -1.2050627 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 0.311      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 265      |\n",
      "|    iterations         | 41800    |\n",
      "|    time_elapsed       | 788      |\n",
      "|    total_timesteps    | 209000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -48.5    |\n",
      "|    explained_variance | 0.00595  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 41799    |\n",
      "|    policy_loss        | 162      |\n",
      "|    reward             | 2.872833 |\n",
      "|    std                | 1.29     |\n",
      "|    value_loss         | 14       |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 41900      |\n",
      "|    time_elapsed       | 790        |\n",
      "|    total_timesteps    | 209500     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 41899      |\n",
      "|    policy_loss        | -149       |\n",
      "|    reward             | -1.6821623 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 12         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 265        |\n",
      "|    iterations         | 42000      |\n",
      "|    time_elapsed       | 792        |\n",
      "|    total_timesteps    | 210000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 41999      |\n",
      "|    policy_loss        | 203        |\n",
      "|    reward             | 0.09118791 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 22.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 265       |\n",
      "|    iterations         | 42100     |\n",
      "|    time_elapsed       | 794       |\n",
      "|    total_timesteps    | 210500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42099     |\n",
      "|    policy_loss        | -101      |\n",
      "|    reward             | 0.4624235 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 7.47      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 42200     |\n",
      "|    time_elapsed       | 796       |\n",
      "|    total_timesteps    | 211000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42199     |\n",
      "|    policy_loss        | -70.5     |\n",
      "|    reward             | -5.136126 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 2.89      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 42300     |\n",
      "|    time_elapsed       | 798       |\n",
      "|    total_timesteps    | 211500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42299     |\n",
      "|    policy_loss        | -0.284    |\n",
      "|    reward             | 1.0626558 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 0.253     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 42400       |\n",
      "|    time_elapsed       | 800         |\n",
      "|    total_timesteps    | 212000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -48.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 42399       |\n",
      "|    policy_loss        | 91.4        |\n",
      "|    reward             | -0.33602154 |\n",
      "|    std                | 1.3         |\n",
      "|    value_loss         | 5.55        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 42500     |\n",
      "|    time_elapsed       | 802       |\n",
      "|    total_timesteps    | 212500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42499     |\n",
      "|    policy_loss        | -140      |\n",
      "|    reward             | 1.9956255 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 9.87      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 42600     |\n",
      "|    time_elapsed       | 804       |\n",
      "|    total_timesteps    | 213000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42599     |\n",
      "|    policy_loss        | 9.69      |\n",
      "|    reward             | 1.0398699 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 0.317     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 42700     |\n",
      "|    time_elapsed       | 806       |\n",
      "|    total_timesteps    | 213500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42699     |\n",
      "|    policy_loss        | -716      |\n",
      "|    reward             | 6.1373057 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 257       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 42800      |\n",
      "|    time_elapsed       | 808        |\n",
      "|    total_timesteps    | 214000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.7      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 42799      |\n",
      "|    policy_loss        | -1.65e+03  |\n",
      "|    reward             | -12.559274 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 1.44e+03   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 42900       |\n",
      "|    time_elapsed       | 810         |\n",
      "|    total_timesteps    | 214500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -48.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 42899       |\n",
      "|    policy_loss        | -102        |\n",
      "|    reward             | -0.58773464 |\n",
      "|    std                | 1.31        |\n",
      "|    value_loss         | 5.84        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 43000      |\n",
      "|    time_elapsed       | 812        |\n",
      "|    total_timesteps    | 215000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 42999      |\n",
      "|    policy_loss        | -66.1      |\n",
      "|    reward             | -0.8804246 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 1.92       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 43100     |\n",
      "|    time_elapsed       | 815       |\n",
      "|    total_timesteps    | 215500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 43099     |\n",
      "|    policy_loss        | 73        |\n",
      "|    reward             | 0.9736069 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 2.86      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 43200     |\n",
      "|    time_elapsed       | 817       |\n",
      "|    total_timesteps    | 216000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 43199     |\n",
      "|    policy_loss        | 38.5      |\n",
      "|    reward             | -1.289092 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 3.01      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 264      |\n",
      "|    iterations         | 43300    |\n",
      "|    time_elapsed       | 819      |\n",
      "|    total_timesteps    | 216500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -48.7    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 43299    |\n",
      "|    policy_loss        | 72.2     |\n",
      "|    reward             | -0.93848 |\n",
      "|    std                | 1.31     |\n",
      "|    value_loss         | 3.72     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 264       |\n",
      "|    iterations         | 43400     |\n",
      "|    time_elapsed       | 821       |\n",
      "|    total_timesteps    | 217000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -48.8     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 43399     |\n",
      "|    policy_loss        | -48.8     |\n",
      "|    reward             | 0.411452  |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 1.26      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 264         |\n",
      "|    iterations         | 43500       |\n",
      "|    time_elapsed       | 823         |\n",
      "|    total_timesteps    | 217500      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -48.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 43499       |\n",
      "|    policy_loss        | 25.6        |\n",
      "|    reward             | -0.30132204 |\n",
      "|    std                | 1.31        |\n",
      "|    value_loss         | 0.399       |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trained_a2c \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_a2c\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43ma2c\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500000\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m if_using_a2c \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/stock/lib/python3.10/site-packages/finrl/agents/stablebaselines3/models.py:103\u001B[0m, in \u001B[0;36mDRLAgent.train_model\u001B[0;34m(self, model, tb_log_name, total_timesteps)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_model\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, tb_log_name, total_timesteps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5000\u001B[39m):\n\u001B[0;32m--> 103\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTensorboardCallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/Desktop/stock/lib/python3.10/site-packages/stable_baselines3/a2c/a2c.py:194\u001B[0m, in \u001B[0;36mA2C.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfA2C,\n\u001B[1;32m    187\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    192\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    193\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfA2C:\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/stock/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[0;32m--> 259\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m continue_training \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    262\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/stock/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:220\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_episode_starts \u001B[38;5;241m=\u001B[39m dones\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;66;03m# Compute value for the last timestep\u001B[39;00m\n\u001B[0;32m--> 220\u001B[0m     values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_as_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_obs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    222\u001B[0m rollout_buffer\u001B[38;5;241m.\u001B[39mcompute_returns_and_advantage(last_values\u001B[38;5;241m=\u001B[39mvalues, dones\u001B[38;5;241m=\u001B[39mdones)\n\u001B[1;32m    224\u001B[0m callback\u001B[38;5;241m.\u001B[39mon_rollout_end()\n",
      "File \u001B[0;32m~/Desktop/stock/lib/python3.10/site-packages/stable_baselines3/common/policies.py:724\u001B[0m, in \u001B[0;36mActorCriticPolicy.predict_values\u001B[0;34m(self, obs)\u001B[0m\n\u001B[1;32m    717\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    718\u001B[0m \u001B[38;5;124;03mGet the estimated values according to the current policy given the observations.\u001B[39;00m\n\u001B[1;32m    719\u001B[0m \n\u001B[1;32m    720\u001B[0m \u001B[38;5;124;03m:param obs: Observation\u001B[39;00m\n\u001B[1;32m    721\u001B[0m \u001B[38;5;124;03m:return: the estimated values.\u001B[39;00m\n\u001B[1;32m    722\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    723\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mextract_features(obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvf_features_extractor)\n\u001B[0;32m--> 724\u001B[0m latent_vf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp_extractor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_critic\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    725\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_net(latent_vf)\n",
      "File \u001B[0;32m~/Desktop/stock/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:228\u001B[0m, in \u001B[0;36mMlpExtractor.forward_critic\u001B[0;34m(self, features)\u001B[0m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward_critic\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: th\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m th\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m--> 228\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/stock/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/stock/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/Desktop/stock/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/stock/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=500000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 45         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 44         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.31128678 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015403882 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0119     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.48        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    reward               | 0.5735508   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 15          |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3175090.47\n",
      "total_reward: 2175090.47\n",
      "total_cost: 337808.39\n",
      "total_trades: 80668\n",
      "Sharpe: 0.689\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015230103 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00311     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    reward               | -1.4208665  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 65.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019708812 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00785    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    reward               | 4.271296    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 45.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 44        |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 231       |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0166731 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -41.4     |\n",
      "|    explained_variance   | -0.0536   |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 8.35      |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.0212   |\n",
      "|    reward               | 2.4547098 |\n",
      "|    std                  | 1.01      |\n",
      "|    value_loss           | 19.9      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 279         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018124811 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.000224    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    reward               | 2.7911348   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 76.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 325         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024045099 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.00731    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.2        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    reward               | 1.2000475   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 85.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 401         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020597177 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.00494    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.9        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    reward               | 1.162485    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 39          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 457         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019878255 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.00339     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    reward               | -0.45392472 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 57          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 498         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017024204 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | -0.0128     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.4        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    reward               | 1.0464087   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 79.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 542         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019309249 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.00221     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 41.1        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    reward               | 4.342377    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 90.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 595         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023128178 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | -0.0209     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.7        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | -0.17601006 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 52.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 644         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.0221886   |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.00158     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    reward               | -0.01640089 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 87.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 686         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021495752 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.00679     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00989    |\n",
      "|    reward               | -3.5598922  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 730         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020674404 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.0104      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    reward               | 6.633994    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 50.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 780         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020497315 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.00979     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 55.1        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | 0.8197784   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 76          |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5135262.34\n",
      "total_reward: 4135262.34\n",
      "total_cost: 287991.52\n",
      "total_trades: 76232\n",
      "Sharpe: 0.825\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 824         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016542673 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.0115      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 57.3        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00886    |\n",
      "|    reward               | 0.7494548   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 902         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015852435 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.00322     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 53.8        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00988    |\n",
      "|    reward               | 0.030309012 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 948         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018456738 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | 0.0132      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23          |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    reward               | 3.2421417   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 59.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 41         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 991        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02275483 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42        |\n",
      "|    explained_variance   | 0.00528    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 75.2       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.00876   |\n",
      "|    reward               | 0.12529488 |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 120        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 1038        |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016805276 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.0179      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 91.7        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00777    |\n",
      "|    reward               | -7.0438967  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 151         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 1084        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023760147 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.0146      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23          |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.007      |\n",
      "|    reward               | 2.0401194   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 51.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1128        |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020118829 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.0153      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.2        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    reward               | 0.71143794  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 77.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1174        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016950518 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.0019      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 48.3        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    reward               | 6.65837     |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 1221        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015065431 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.000935    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 63          |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    reward               | 0.15126267  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 41         |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 1270       |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02162559 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.3      |\n",
      "|    explained_variance   | -0.051     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 12.2       |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.00781   |\n",
      "|    reward               | 2.7743616  |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 35.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 1316        |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018873222 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | -0.00448    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 150         |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    reward               | -0.3141937  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 403         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 1365        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019432027 |\n",
      "|    clip_fraction        | 0.0966      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | -0.00524    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 154         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    reward               | -3.5503688  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 576         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 1415        |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016036455 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.0583      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0097     |\n",
      "|    reward               | -0.7532961  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 28          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 1465        |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020757344 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.00572     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 51.6        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    reward               | 1.4703931   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 94.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 1526        |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011127746 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0169      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 231         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | -1.467241   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 210         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3243555.19\n",
      "total_reward: 2243555.19\n",
      "total_cost: 285552.40\n",
      "total_trades: 75997\n",
      "Sharpe: 0.630\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 1580        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016851306 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0239      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.8        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00735    |\n",
      "|    reward               | 0.46168545  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 38.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 1635        |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020944998 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0675      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 40.7        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    reward               | -0.6691587  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 98          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 1690        |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019425632 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.00129     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 683         |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    reward               | 0.7678009   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 779         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 40          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 1749        |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029058285 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.0135      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.1        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    reward               | 1.0060796   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 92.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 1797        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020738672 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0737      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | -3.2037091  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 26.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 41          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 1827        |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02297673  |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0195      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 81.3        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    reward               | -0.07032298 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 41         |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 1855       |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02278569 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.0599     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 58         |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    reward               | -8.025124  |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 56         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 1883        |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012080987 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0106      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.7        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    reward               | -1.7869323  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 33.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 1912        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018431693 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0313      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | -0.90353096 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 70.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 43         |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 1941       |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03000924 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.0217     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 36.4       |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.00702   |\n",
      "|    reward               | 0.6959806  |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 66         |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 1969        |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02106736  |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0226      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.9        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00763    |\n",
      "|    reward               | -0.07551384 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 78.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 44         |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 1997       |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02621992 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.9      |\n",
      "|    explained_variance   | 0.0432     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 6.73       |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    reward               | -0.5257798 |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 16.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 2029        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018179985 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0481      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0094     |\n",
      "|    reward               | 0.07973856  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 50.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 2057        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025584307 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.0137      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.7        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | -1.5210438  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 45.8        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2434835.58\n",
      "total_reward: 1434835.58\n",
      "total_cost: 292094.35\n",
      "total_trades: 75755\n",
      "Sharpe: 0.531\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 2085        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020613633 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | -0.0237     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    reward               | -6.6412554  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 20          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 2115        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051772773 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.0464      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | 0.9050805   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 28.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 45         |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 2145       |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02411993 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.2      |\n",
      "|    explained_variance   | 0.0566     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 24.1       |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    reward               | 8.264698   |\n",
      "|    std                  | 1.07       |\n",
      "|    value_loss           | 77.9       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 2177         |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0139401145 |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.2        |\n",
      "|    explained_variance   | 0.042        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 61           |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    reward               | 0.5889933    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 99.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 2209        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017555572 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.00983     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.24        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00979    |\n",
      "|    reward               | 1.2748959   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 27          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 2240        |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024905618 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.0205      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 107         |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    reward               | -0.38472286 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 183         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 2272        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008009402 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.00851     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 110         |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | -8.162935   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 358         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 2303        |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011461023 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.0416      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 33.1        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | 0.9152776   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 70.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 2335        |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016434327 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.00823     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 157         |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00802    |\n",
      "|    reward               | 2.981041    |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 383         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 2365        |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016391305 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0388      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 152         |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00422    |\n",
      "|    reward               | 6.4678645   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 327         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 2396        |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012791837 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0564      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 72.7        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00704    |\n",
      "|    reward               | 3.231686    |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 161         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 2428        |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018063571 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0386      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 103         |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | -0.81493115 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 293         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 2457        |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014747455 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 47.7        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00979    |\n",
      "|    reward               | 0.52013004  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 222         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 2488        |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021737006 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.0135      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 70.8        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00868    |\n",
      "|    reward               | -7.884881   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4069059.76\n",
      "total_reward: 3069059.76\n",
      "total_cost: 264245.13\n",
      "total_trades: 72011\n",
      "Sharpe: 0.683\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 48          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 2517        |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056785204 |\n",
      "|    clip_fraction        | 0.422       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | 0.00132     |\n",
      "|    reward               | -0.21071224 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 36.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 2547        |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017078368 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.0117      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 82          |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00869    |\n",
      "|    reward               | 1.1230668   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 218         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 2577        |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027008304 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.0505      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 43.6        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00935    |\n",
      "|    reward               | 6.6512985   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 66.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 2609        |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039094795 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.0596      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00381    |\n",
      "|    reward               | 3.7435694   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 37.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 2640        |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023921957 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.0723      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.3        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    reward               | 0.22937617  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 86.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 49         |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 2670       |\n",
      "|    total_timesteps      | 133120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01888536 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.8      |\n",
      "|    explained_variance   | 0.126      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 27.6       |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    reward               | -0.9992001 |\n",
      "|    std                  | 1.1        |\n",
      "|    value_loss           | 87.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 2700        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024767734 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 87.1        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    reward               | 3.9526186   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 162         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 2729        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060827535 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.0572      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.93        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | 0.00566     |\n",
      "|    reward               | -1.1473198  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 21.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 2760        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014017103 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 53.8        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00271    |\n",
      "|    reward               | -0.07142399 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 72.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 2791        |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020744808 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.153       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.2        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00391    |\n",
      "|    reward               | 0.70842856  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 59          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 50         |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 2822       |\n",
      "|    total_timesteps      | 143360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02585392 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44        |\n",
      "|    explained_variance   | 0.0351     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 13.1       |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.00977   |\n",
      "|    reward               | -1.950569  |\n",
      "|    std                  | 1.11       |\n",
      "|    value_loss           | 29.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 50          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 2852        |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032436244 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 48.5        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00307    |\n",
      "|    reward               | -1.1932291  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 84.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 2883        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015441446 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.3        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0051     |\n",
      "|    reward               | -9.417746   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 69.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 2913        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012106962 |\n",
      "|    clip_fraction        | 0.0806      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00548    |\n",
      "|    reward               | 0.49566022  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 42.9        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1980690.34\n",
      "total_reward: 980690.34\n",
      "total_cost: 169265.73\n",
      "total_trades: 66162\n",
      "Sharpe: 0.369\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 2943        |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021132182 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.0802      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.5         |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00613    |\n",
      "|    reward               | -1.626163   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 36.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 2973        |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024074746 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.3        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0007     |\n",
      "|    reward               | 1.8852057   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 48.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 3003        |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019135812 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 75.2        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    reward               | 4.1208997   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 3033        |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049443997 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.0738      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.9        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00461    |\n",
      "|    reward               | -0.5149379  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 31.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 3065        |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017629089 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 31.2        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00908    |\n",
      "|    reward               | 2.1882532   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 81.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 52         |\n",
      "|    iterations           | 79         |\n",
      "|    time_elapsed         | 3094       |\n",
      "|    total_timesteps      | 161792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01690145 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.3      |\n",
      "|    explained_variance   | 0.155      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 42.8       |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.00397   |\n",
      "|    reward               | 8.179026   |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 108        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 3124        |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028971553 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.0989      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.7        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    reward               | 0.30852684  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 65          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 3152        |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019610118 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.0958      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00749    |\n",
      "|    reward               | -0.4584207  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 3180        |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015684314 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.0692      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 35.4        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    reward               | -0.6564304  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 157         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 3209        |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021742439 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.0788      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    reward               | 0.8954489   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 3237        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032015637 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.00259     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.17        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    reward               | -0.3945083  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 85         |\n",
      "|    time_elapsed         | 3264       |\n",
      "|    total_timesteps      | 174080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05891064 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.4      |\n",
      "|    explained_variance   | 0.0777     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 15.1       |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0085    |\n",
      "|    reward               | 0.49847165 |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 58.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 3291        |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030904215 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.082       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | -0.07042978 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 77.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 3318        |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050372817 |\n",
      "|    clip_fraction        | 0.459       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.034       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.26        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | 0.00687     |\n",
      "|    reward               | 2.851221    |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 21.8        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2677060.35\n",
      "total_reward: 1677060.35\n",
      "total_cost: 269589.12\n",
      "total_trades: 72774\n",
      "Sharpe: 0.583\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 3346        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028356094 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.2        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    reward               | -2.7264383  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 35.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 54           |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 3373         |\n",
      "|    total_timesteps      | 182272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.05962711   |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.6        |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 20.5         |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | 0.00107      |\n",
      "|    reward               | -0.056611504 |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 36.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 3402        |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024442453 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.0674      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00332    |\n",
      "|    reward               | -0.41769487 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 43          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 3433        |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034742087 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | -0.00378    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.72        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    reward               | -0.20895879 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 14          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 3463        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031162115 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.0296      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 62.7        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00606    |\n",
      "|    reward               | -2.0798807  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 68.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 3490       |\n",
      "|    total_timesteps      | 190464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04095839 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.9      |\n",
      "|    explained_variance   | -0.000793  |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 25.5       |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    reward               | -1.9883596 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 89.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 3519        |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023274139 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | -0.0252     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.7        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00922    |\n",
      "|    reward               | 0.616459    |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 35.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 3547        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030782362 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.00162     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 96.8        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    reward               | 0.033399694 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 154         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 3575        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027262196 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 42.3        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    reward               | 2.1065326   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 68.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 3603       |\n",
      "|    total_timesteps      | 198656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04975403 |\n",
      "|    clip_fraction        | 0.407      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45        |\n",
      "|    explained_variance   | 0.0158     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 15.2       |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -9.3e-05   |\n",
      "|    reward               | 0.73203063 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 31.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 3632        |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028849795 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.0384      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 31.8        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00741    |\n",
      "|    reward               | 0.098063245 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 53.4        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
